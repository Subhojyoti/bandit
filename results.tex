	
	Here, we state the main theorem of the paper which shows the regret upper bound of ClusUCB.
	
\begin{theorem}
The upper bound on the total regret over horizon $T$ after round $m$ is given by 
$R_{T}\leq \sum_{i\in B_{m}}\bigg (\max{\bigg\lbrace \bigg(\dfrac{32}{(\Delta_{i})^{3}}\bigg) ,\bigg(\dfrac{25\Delta_{i}}{(\Delta^{2})(0.16T\Delta^{2})^{2|B_{m}|^{2}\Delta/5}}\bigg)\bigg\rbrace} + \bigg(\Delta_{i}+\dfrac{32\log{(T\dfrac{\Delta_{i}^{4}}{16})}}{\Delta_{i}}\bigg)\bigg)$, 
where $A$ is the set of arms and $\Delta$ is the minimal gap.
\end{theorem}

\begin{proof}
	See Appendix.
\end{proof}

\begin{remark}
	Thus, we see the most significant term in the regret is $\dfrac{32\log{(T\dfrac{\Delta_{i}^{4}}{12})}}{\Delta_{i}}$, which is significantly lower than UCB1, UCB2, EXP3, UCB($\delta$), MOSS, UCB-Revisited and Median Elimination under certain cases when the $\Delta \rightarrow 0$ and $K$ is large. Also ClusUCB is more efficient than UCB1, MOSS, EXP3, UCB($\delta$), KL-UCB as $K$ scales up because being a round-based algorithm, it is removing sub-optimal arms in each round as opposed to the former algorithms which are calculating the confidence bounds over all arms in each timestep and then choosing the max of them. Also evident from the proof is that if the horizon $T$ is small or very large we can tune the parameter $\psi(m)$ carefully to have a lesser regret. For readers convenience a table is given in Appendix G on the various regret bounds of several algorithms. 
\end{remark}	


\todos{(Subho)To follow the same way as UCB-Revisited proof, did away with the event $\xi_{1},\xi_{2},\xi_{3}$}

%In this section we will prove the bounds based on the events $\xi_{1}$,$\xi_{2}$ and $\xi_{3}$. In $\xi_{1}$, we will assume two important assumptions $i)\hat{r}^{*}<\hat{r}_{i},\forall i\in s_{i}$ and $ii)\exists a_{i}\in s_{i}$ such that $\sqrt{\dfrac{\epsilon_{m}}{w_{s_{i}}}}<\dfrac{\Delta_{i}}{5}$. For $\xi_{2}$, we will assume that $a^{*}\in s^{*}$ and $|s^{*}|=1$, $a_{i}\in s_{i} \forall a_{i}\setminus a^{*}\in B_{m}$ and $\exists a_{max_{s_{i}}}$ such that $\sqrt{\epsilon_{m}}<\dfrac{2\Delta_{s}}{5}$, where $\Delta_{s}=r^{*}-r_{max_{s_{i}}}$ and $\hat{r}_{max_{s_{i}}}>\hat{r}_{i}, \forall i\in s_{i}$. $\xi_{3}$ be the event when the optimal arm $a^{*}$ gets eliminated by a sub-optimal arm. At the start of any round $m$, we fix $\epsilon_{m}$.

\begin{remark}
A sketch of the proof is given here. In the first step, we try to calculate the number of pulls $n_{s_{i}}$ required to make the optimal arm  safe with a high probability so that it goes up atleast $\dfrac{\hat{\Delta}_{s_{i}}}{2}$ where $\hat{\Delta}_{s_{i}}=max_{i\in s_{i}}\hat{r}_{i}-min_{j\in s_{i}}\hat{r}_{j},i\neq j$. This is shown in Proposition 1. Second step, we try to bound the probability of arm elimination of any sub-optimal arm within a cluster. We try to find out the event which will lead to the elimination of an arm within a cluster. This is shown in Proposition 2. Third step, we try to bound the probability of cluster elimination with all arms within it and the favourable event leading to it. This is shown in Proposition 3. Finally, in the proof of Theorem 1 we combine all these to get the regret bound.  
\end{remark}
	
\begin{proposition}
The probability that the optimal arm $a^{*}\in s_{i}$ will lie above $\hat{r}_{min_{s_{i}}}+ \dfrac{\hat{\Delta}_{s_{i}}}{2}$ after $\bigg\lceil\dfrac{2\log (\psi(m)T\epsilon_{m}^{2})}{\epsilon_{m}}\bigg\rceil$ pulls in the $m$-th round is given by $\bigg\lbrace 1- \dfrac{1}{(\psi(m)T\epsilon_{m}^{2})^{\ell_{m}^{2}\epsilon_{m}}} \bigg\rbrace$ where $\hat{r}_{min_{s_{i}}}$ is the minimum payoff in $s_{i}$, $\hat{\Delta}_{s_{i}}=max_{i\in s_{i}}\hat{r}_{i}-min_{j\in s_{i}}\hat{r}_{j}, i\neq j$, $\epsilon_{m}$ is halved after every round and $T$ is the horizon. 
\end{proposition}

%$\epsilon_{m}=\max{\bigg\lbrace\dfrac{\hat{\Delta}_{m}}{\ell_{m}} \dfrac{2}{\sqrt{\psi{(m)T}}}\bigg\rbrace}$

	The proof of Proposition 1 is given in \textbf{Appendix A}.(Supplementary material)

\begin{remark}
	Thus, we see that as the agent falls through rounds, with increasing values of $\ell_{m}$ the probability of optimal arm $a^{*}$ lying above $\dfrac{\hat{\Delta}_{s_{i}}}{2}$ increases as $\ell_{m}^{2}\epsilon_{m}\geq 4$ (as, $\epsilon_{m}$ is halved after every round and $\ell_{m}$ is doubled) increases with increasing $\ell_{m}$. But, $\ell_{m}$ is bounded by $D_{m}$ and say after the $m_{D}$ round the probability of optimal arm staying above the specified range gets bounded by $\bigg\lbrace 1- \dfrac{1}{(\psi(m)T\epsilon_{m}^{2})^{D_{m}^{2}\epsilon_{m}}} \bigg\rbrace$. But we know from the definition of $D_{m}$ that $ D_{m}=\bigg\lceil(\dfrac{1}{\epsilon_{m}})^{1/2}\bigg\rceil$ or $D^{2}\epsilon_{m}\approx 1$. Hence, the probability that   $a^{*}$ after $n_{s_{i}}$ pulls in round $m_{D}$ going above $\dfrac{\hat{\Delta}_{s_{i}}}{2}$ is $\bigg\lbrace 1- \dfrac{1}{(\psi(m)T\epsilon_{m}^{2}) }\bigg\rbrace$.

%	We must also point out that $\epsilon_{m}$ can become arbitrarily small, so the value $\log{(\psi{(m)}T\epsilon_{m}^{2})}$ can become negative. To guard against that scenario we have setup a tolerance level such as $\epsilon_{m}>\dfrac{2}{\sqrt{\psi{(m)}T}}$ and below this value $\epsilon_{m}$ should not be allowed to fall. 
	We also see that $\ell_{m}$ is doubled after every round, independent of the rise in $D_{m}$ and as $\epsilon_{m}\rightarrow \Delta $, $\ell_{m}$ gets upper bounded by $D_{m}$ and will rise no more and hence $D_{m}$ also gets fixed as $D_{m}^{2}\epsilon_{m}\approx 1$ for any round $m$. We make the $D_{m}$ increase with the rounds as it controls our rate of exploration and in the later rounds we need more exploration as arms close to the optimal arm will survive till the later rounds and the algorithm needs to discriminate amongst them.  Also, if $\epsilon_{m}$ is very large, then $D_{m}$ becomes very small and consequently there is very small exploration. Hence, $D_{m}$ is the maximum of $\lbrace \dfrac{1}{\sqrt{\epsilon_{m}}}, K \rbrace$, which ensures the algorithm does sufficient exploration. 
\end{remark}


\todos{(Subho) Merged proposition 2 and 3 into one to follow the UCB-Revisited case (a)}

\begin{proposition}
The number of times an arm $a_{i}\in s_{i}$ is pulled in each round is $n_{s_{i}}=\bigg\lceil\dfrac{2\log{(\psi(m)T\epsilon_{m}^{2})}}{\epsilon_{m}}\bigg\rceil$ and this eliminates the arm $a_{i}$ such that $\sqrt{\epsilon_{m}}<\dfrac{\Delta_{i}}{2}$ by the condition $\bigg\lbrace\hat{r}_{i} + \sqrt{\dfrac{\log (\psi(m)T\epsilon_{m}^{2})}{2w_{s_{i}} n_{s_{i}}}} < \max_{j\in s_{i}}\hat{r}_{j} - \sqrt{\dfrac{\log (\psi(m)T\epsilon_{m}^{2})}{2w_{s_{i}} n_{s_{i}}}} \bigg\rbrace, \forall s_{i}\in S$ with probability $\bigg\lbrace 1-\bigg(\dfrac{2}{\psi(m)T\epsilon_{m}^{2}}\bigg)\bigg\rbrace$. 
%where $B_{m}$ is the set of arms still not eliminated in the $m$-th round.
%- \sqrt{\dfrac{(\log{(4(\psi(m)T\epsilon_{m}^{2})}}{2w\ell_{m}n_{s_{i}}}}
\end{proposition}

	The proof of Proposition 2 is given in \textbf{Appendix B}.(Supplementary material).

\begin{remark}
	
	Thus, we see that the confidence interval term $c_{m}=\sqrt{\dfrac{\log (\psi(m)T\epsilon_{m}^{2})}{2w_{s_{i}} n_{s_{i}}}}$ makes the algorithm eliminate an arm $a_{i}$ as soon as $\sqrt{\epsilon_{m}}<\dfrac{\Delta_{i}}{2}$. The above result is in contrast with UCB-Revisited which only deletes an arm if $\tilde{\Delta}_{m}<\dfrac{\Delta_{i}}{2}$, where $\tilde{\Delta}_{m}$ is initialized at $1$ and is halved after every round. We also see from our result that a much less stricter elimination condition is adopted inside a cluster. We are aggressively eliminating inside a cluster because we are exploring locally and we are guaranteed with  $\bigg\lbrace 1- \dfrac{2}{(\psi(m)T\epsilon_{m}^{2})^{\ell_{m}^{2}\epsilon_{m}}} \bigg\rbrace$ probability that in the $m$-th round optimal arm $a^{*}$ will atleast lie above $\hat{r}_{min_{s_{i}}}+ \hat{\Delta}_{s_{i}}$. Also, the weight $w_{s_{i}}$ as shown in the proofs actually help us in faster elimination of arms by the condition $ii$ in $\xi_
{1}$ explained at the start of the proof. The higher the weight faster is arm elimination but it also increases error probability which might actually lead to a higher cumulative regret. The weight $w_{s_{i}}$ depends on the cluster size and the larger the cluster size, the higher will be the weight increasing the probability of arm elimination as we want smaller and smaller number of elements in clusters so that we can discriminate effectively amongst the clusters.
%\end{remark}
%
%
%\begin{proposition}
%With a probability of $\bigg\lbrace 1-\bigg(\dfrac{2}{\psi(m)T\epsilon_{m}^{2})}\bigg)\bigg\rbrace$ a sub-optimal arm can be deleted within a cluster $s_{i}$ in round $m$ by the arm elimination condition, where $\epsilon_{m}=\max{\bigg\lbrace\dfrac{\hat{\Delta}_{s,m}}{\ell_{m}}, \dfrac{1}{\sqrt{\psi{(m)T}}}\bigg\rbrace}$, if $\hat{\Delta}_{s,m}\neq 0$ and $T$ is the horizon.
%\end{proposition}
%
%	The proof of Proposition 3 is given in \textbf{Appendix C}.(Supplementary material)	
%
%\begin{remark}
	Thus, we see that the probability of a sub-optimal arm $a_{i}$ is eliminated in $\xi_{1}$ is  $\bigg(1-\dfrac{2}{\psi(m)T\epsilon_{m}^{2})}\bigg)$ which increases as the algorithm falls through the round as $\epsilon_{m}$ keeps on decreasing.
	
	Due to the initial uncertainty we can sufficiently tune $\psi(m)$ to increase our pulls in the initial rounds in a bounded fashion. But this function can be set arbitrarily high resulting in a skewed regret upper bound and so we need to define a structure and bound this function as well. We define $\psi(m)=\dfrac{c}{m}$ where $c>0, m\geq 1$ as defined previously. Notice also that $\psi(m)$ is a monotonically decreasing function and for any round $m$, $|\psi(m+1)|\leq |\psi(m)|$. 
%In experiment 2 and 3 where $\psi(m)=K^{5/m}$ is defined since the variance is very high in those cases. 
%Since $|\psi(m)|\geq 0,\forall m$, this also leads to increase in the probability of arm deletion as compared to UCB-Revisited.
When the time horizon $T$ is very large or very small, as the pulls $n_{s_{i}}$ depends on $T$, we can tune $\psi(m)$ by changing $c$ so that the exploration remains balanced. Since we are exploring locally and the pulls are increasing after every round so we decrease $\psi(m)$ in the later rounds which not only tapers down the growth of $n_{s_{i}}$ but also decreases arm elimination probability, as in the later rounds, only arms closer to the optimal arm survive and we need careful elimination.
\end{remark}
	
	
	

\begin{proposition}
With a probability of $\bigg(1-\dfrac{4}{(\psi(m)T\epsilon_{m}^{2})^{1+|B_{m}|^{2}\epsilon_{m}}}\bigg)$ a sub-optimal arm can be deleted in round $m$, where $\hat{\Delta}_{m}=\max_{i\in B_{m}}{\lbrace\hat{r}_{i}\rbrace}-\min_{j\in B_{m}}{\lbrace\hat{r}_{j}\rbrace}$,  $\epsilon_{m}=\max{\bigg\lbrace\dfrac{\hat{\Delta}_{m}}{\ell_{m}}, \dfrac{1}{\sqrt{\psi{(m)T}}}\bigg\rbrace}$, $B_{m}$ is the set of arms still not eliminated in the $m$-th round and $T$ is the horizon.
\end{proposition}

	The proof of Proposition 3 is given in \textbf{Appendix C}.(Supplementary material)

\todos{(Subho) Have to re-write the remark once this proposition is proved}
\begin{remark}
	Thus, from the conditions imposed on $\xi_{1}$ and $\xi_{2}$ we see that the cluster deletion condition is more tightly coupled than arm elimination condition. This is because, for cluster elimination the condition depends on the number of arms surviving till $m$-th round that is $|B_{m}|$ which itself depends on the pulls $n_{s_{i}}$ and cluster size limit $D_{m}$. We also point out that at any round $m$ there will be $|S_{m}|$ arm elimination conditions because each cluster has its own arm elimination condition weighted by $w_{s_{i}}$, resulting in the probability of $\max{\bigg\lbrace \bigg(\dfrac{2}{\psi(m)T\epsilon_{m}^{2})}\bigg) ,\bigg(\dfrac{4}{(\psi(m)T\epsilon_{m}^{2})^{1+|B_{m}|^{2}\epsilon_{m}}}\bigg)\bigg\rbrace}$ of being eliminated, an increase over UCB-Revisited and Median Elimination. This is because each arm can either be eliminated by the the arm elimination condition within a cluster or by the cluster elimination condition whereby all the arms within a cluster are eliminated. Also, we 
see that lower bounding $\epsilon_{m}=\dfrac{2}{\sqrt{\psi(m)T}}$, lower bounds the probability of arm elimination atleast $0.5$ in $\xi_{1}$ and in $\xi_{2}$ the probability of cluster elimination and stopping very close to $1$.
\end{remark}

%\begin{theorem}
%The upper bound on the total regret over horizon $T$ after round $m$ is given by $R_{T}\leq \sum_{i\in B_{m}}\bigg (\max{\bigg\lbrace \bigg(\dfrac{27}{\psi(m)(\Delta_{i})^{\frac{3}{5}}}\bigg) ,\bigg(\dfrac{25\Delta_{i}}{(\psi(m)\Delta^{2})(0.16\psi(m)T\Delta^{2})^{2|B_{m}|^{2}\Delta/5}}\bigg)\bigg\rbrace} + \bigg(\Delta_{i}+\dfrac{27\log{(\psi(m)T\dfrac{\Delta_{i}^{\frac{8}{5}}}{12})}}{\Delta_{i}^{\frac{3}{5}}}\bigg)\bigg)$, where $B_{m}$ is the set of arms still not eliminated in the $m$-th round and $\Delta$ is the minimal gap.
%\end{theorem}

%First level headings are all caps, flush left, bold and in point size
%12. One line space before the first level heading and 1/2~line space
%after the first level heading.

\subsection{Error Bound}
	
	In this section, we try to come up with an error bound for the algorithm if at any round $m$, the optimal arm $a^{*}$ gets eliminated by another sub-optimal arm. Since, arm elimination condition is the more aggressive elimination condition, we come up with an error bound that bounds the regret once the  optimal arm gets eliminated by another sub-optimal arm. The proof of this directly follows from \textbf{Theorem 1}, and mimics the proof as in \cite{auer2010ucb}.
	
\begin{theorem}
The error bound till round $m$ is given by $e_{t}\leq \sum_{i\in A^{'}}\bigg(\dfrac{51}{\psi(m)\Delta_{i}^{6/5}} \bigg)+\sum_{i\in A^{''}\setminus A^{'}}\bigg(\dfrac{51}{\psi(m)\Delta_{b}^{6/5}} \bigg)$, where the arms surviving till $m$-th round belong to the set $A^{'}$, arms to still survive and eliminate arm $a^{*}$ after round $m$ belong to $A^{''}$.
\end{theorem}
The proof of Theorem 2 is given in \textbf{Appendix F}.(Supplementary material)
%~\ref{App:B}.
