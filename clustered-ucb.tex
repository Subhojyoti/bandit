\documentclass[twoside]{article} 

\usepackage{macros}

\begin{document}

\twocolumn[

\aistatstitle{Improved Regret Bounds with Clustered UCB}

\aistatsauthor{ }

\aistatsaddress{ } ]

\begin{abstract}

In this paper, we present a novel algorithm which achieves a better upper bound on regret for the stochastic multi-armed bandit problem than UCB-Improved and MOSS algorithm and thereby try to answer an open problem which has been raised before. Our proposed method partitions the arms into clusters and then following the UCB-Improved strategy eliminates sub-optimal arms and weak clusters. We corroborate our findings both theoretically and empirically and show that by sufficient tuning of parameters, we can achieve a lower regret than both MOSS and UCB-Improved. In particular, in the test-cases where the arm set is dominated by small $\Delta$ and large $K$ we achieve a significantly lower cumulative regret than these algorithms. We also empirically test our algorithm against many recent algorithms like UCB-V, KL-UCB and DMED and obtain favourable results.

%Old Abstract 27.8.2016

%In this paper, we present a novel algorithm which achieves a better upper bound on regret for the stochastic multi-armed bandit problem then some of the existing algorithms. Our proposed method clusters arms based on their average estimated payoff. This results in a more efficient elimination of arms within clusters as well as the deletion of weak clusters. We prove the regret upper bound  as 
%$\sum_{i\in B_{m}}\bigg (\max{\bigg\lbrace \bigg(\dfrac{32}{(\Delta_{i})^{3}}\bigg) ,\bigg(\dfrac{25\Delta_{i}}{(\Delta^{2})(0.16T\Delta^{2})^{2|B_{m}|^{2}\Delta/5}}\bigg)\bigg\rbrace} + \bigg(\Delta_{i}+\dfrac{32\log{(T\dfrac{\Delta_{i}^{4}}{16})}}{\Delta_{i}}\bigg)\bigg)$, $\Delta$ is the minimal gap between the means of the reward distributions of the optimal arm and a sub-optimal arm, $A$ is the set of arms, $T$ is the horizon and $\psi(m)$ is a parameter of the problem. This bound improves upon the existing algorithm of UCB-Revisited, MOSS, KL-UCB and UCB1 under certain cases. We corroborate our findings both theoretically and empirically and show that by sufficient tuning of parameter, we can achieve a lower regret than all the algorithms mentioned. In particular, in the test-cases where the arm set is dominated by small $\Delta$ and large $K$ we achieve a significantly lower cumulative regret than these algorithms.

%Old Abstract

%In this paper we achieve a lower regret bound by a novel method of using clustering of arms based on their average estimated payoff. By clustering of arms we divide the larger problem into smaller sub-problems and individually solve the sub-problems to arrive at a global solution. We prove that by this method we achieve a regret bound of the order of $O\bigg(K\log(KT\Delta^{2})\bigg)$ where $\Delta$ is the minimal gap between the means of the reward distributions of the optimal arm and a sub-optimal arm , $K$ is the number of arms and $T$ is the horizon. This improves upon UCB-Revisited having regret bound $O\bigg(K\dfrac{\log(T\Delta^{2})}{\Delta}\bigg)$ . This bound also improves upon UCB1 which has a regret of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$ and MOSS which has a minimax distribution free upper bound on the regret in the order of $O(\sqrt{TK})$ and a distribution dependent upper bound of $O\bigg(\dfrac{K\log(T\Delta^{2}/K)}{\Delta}\bigg)$. We corroborate our findings empirically and showed that by sufficient tuning of some parameters we can achieve a lower regret than all the existing algorithms mentioned.
%\dots
%\keywords{Multi-Armed Bandit, UCB, Exploration-Exploitation, Clustering}
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{intro}




\section{Preliminaries}
\label{sec:prelims}
\input{prelims}
	
%Some proofs are given in Appendix A and B. 


%\pagebreak
\section{Clustered UCB algorithm}
\label{sec:clusucb}
\input{algo}

%\newpage
\section{Main results}
\label{sec:results}
\input{results}

\section{Proof Sketch}
\label{sec:proofSketch}
\input{proofSketch}

\section{Simulation experiments}
\label{sec:expts}
\input{expts}


\section{Conclusions and future work}

Our study concludes that theoretically the regret of ClusUCB is lower than UCB1, MOSS and UCB-Improved while empirically we compared against UCB1, MOSS, UCB-Improved, UCB-V, KL-UCB, DMED and Median Elimination under certain cases when the $\Delta_{i}$'s are small and $K$ is large. Such cases can frequently occur in web advertising scenarios where the $r_{i}$ are small and a large number of ads are available in the pool. For the critical case when $\Delta_{i:r_{i}<r*}, \forall i\in A$ are equal then ClusUCB performs better than UCB-Improved, UCB1, UCB-Tuned, KL-UCB, MOSS and Median Elimination with careful choosing  of parameters as shown empirically. Also ClusUCB scales well with large $K$ as compared with UCB1, UCB-Improved, MOSS, KL-UCB and Median Elimination. We must also remember as the number of arms increases UCB1, MOSS, KL-UCB and DMED will take more time as all of these algorithms have to build their confidence set over all the arms whereas algorithms like ClusUCB, UCB-Improved and Median Elimination will take much lesser time as they keep on removing sub-optimal arms after each round. Again KL-UCB because of its calculation of the divergence function performs much slower as compared to ClusUCB. Also, ClusUCB can be used in the budgeted bandit setup since within a fixed horizon/budget $T$, the algorithm comes up with an optimal arm with an exponentially decreasing error probability. Further uses of this algorithm can be in the contextual bandit scenario whereby the clustering of arms can be done on the basis of the feature vectors of the arms(advertisements) and users.
%As $\Delta \rightarrow 0$ and $K$ becomes large Clustered UCB will perform better than KL-UCB. This type of cases frequently occur in web advertising scenarios where the $r_{i}$ are small and large number of ads are available in the pool.
%To check the regret of various algorithms an illustrative table is given below:-
%\newline
%A table showing the relevant regret bounds of some algorithms is shown in Appendix~\ref{App:C}.


\bibliographystyle{plainnat}
%\vspace*{-1cm}
\bibliography{biblio}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\newpage
\onecolumn
\section*{Appendix}

\input{appendix}



\end{document}
