\documentclass[11pt,letterpaper,english]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1in]{geometry}
\usepackage[numbers]{natbib}
\setcitestyle{nocompress}
\bibpunct{(}{)}{;}{a}{}{,}

% \usepackage[nocompress]{cite}
\usepackage{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{authblk}
\usepackage{macros}


\begin{document}
\title{Improved Regret Bounds with Clustered UCB}

\author{}


\renewcommand\Authands{ and }

\date{}

\maketitle

\begin{abstract}
In this paper, we present a novel algorithm which achieves a better upper bound on regret for the stochastic multi-armed bandit problem then some of the existing algorithms. Our proposed method clusters arms based on their average estimated payoff. This results in a more efficient elimination of arms within clusters as well as the deletion of weak clusters. We prove the regret upper bound  as 
$\sum_{i\in A}\bigg (\max{\bigg\lbrace \bigg(\dfrac{27}{c(\Delta_{i})^{\frac{3}{5}}}\bigg) ,\bigg(\dfrac{25\Delta_{i}}{c\Delta^{2})(0.16cT\Delta^{2})^{2\Delta/5}}\bigg)\bigg\rbrace} + \bigg(\Delta_{i}+\dfrac{27\log{(cT\dfrac{\Delta_{i}^{\frac{8}{5}}}{12})}}{\Delta_{i}^{\frac{3}{5}}}\bigg)\bigg)$, where $c>0$ is a constant, $\Delta$ is the minimal gap between the means of the reward distributions of the optimal arm and a sub-optimal arm, $K$ is the number of arms, $T$ is the horizon and $\psi(m)$ is a parameter of the problem. This bound improves upon the existing algorithm of UCB-Revisited, MOSS, KL-UCB and UCB1 under certain cases. We corroborate our findings both theoretically and empirically and show that by sufficient tuning of parameter, we can achieve a lower regret than all the algorithms mentioned. In particular, in the test-cases where the arm set is dominated by small $\Delta$ and large $K$ we achieve a significantly lower cumulative regret than these algorithms.

%Old Abstract

%In this paper we achieve a lower regret bound by a novel method of using clustering of arms based on their average estimated payoff. By clustering of arms we divide the larger problem into smaller sub-problems and individually solve the sub-problems to arrive at a global solution. We prove that by this method we achieve a regret bound of the order of $O\bigg(K\log(KT\Delta^{2})\bigg)$ where $\Delta$ is the minimal gap between the means of the reward distributions of the optimal arm and a sub-optimal arm , $K$ is the number of arms and $T$ is the horizon. This improves upon UCB-Revisited having regret bound $O\bigg(K\dfrac{\log(T\Delta^{2})}{\Delta}\bigg)$ . This bound also improves upon UCB1 which has a regret of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$ and MOSS which has a minimax distribution free upper bound on the regret in the order of $O(\sqrt{TK})$ and a distribution dependent upper bound of $O\bigg(\dfrac{K\log(T\Delta^{2}/K)}{\Delta}\bigg)$. We corroborate our findings empirically and showed that by 
sufficient tuning of some parameters we can achieve a lower regret than all the existing algorithms mentioned.
%\dots
%\keywords{Multi-Armed Bandit, UCB, Exploration-Exploitation, Clustering}
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{intro}




\section{Preliminaries}
\label{sec:prelims}
\input{prelims}
	
%Some proofs are given in Appendix A and B. 


%\pagebreak
\section{Clustered UCB algorithm}
\label{sec:clusucb}
\input{algo}

%\newpage
\section{Main results}
\label{sec:results}
\input{results}


\section{Simulation experiments}
\label{sec:expts}
\input{expts}


\section{Conclusions and future work}
Our study concludes that the regret of ClusUCB is lower than UCB1, UCB2, EXP3, MOSS, UCB-Revisited, KL-UCB, UCB-Tuned, DMED and Median Elimination under certain cases when the $\Delta \rightarrow 0$ and $K$ is large. Such cases can frequently occur in web advertising scenarios where the $r_{i}$ are small and a large number of ads are available in the pool. We see that the upper bound of ClusUCB has $\log$ dependence on horizon $T$ whereas MOSS has a dependence of $T^{1/2}$ in the distribution free case. For the critical case when $\Delta_{i:r_{i}<r*}, \forall i\in A$ are equal then ClusUCB performs better than UCB-Revisited, UCB1, UCB-Tuned, KL-UCB, MOSS and EXP3 with sufficient tuning of parameters as shown empirically. Also ClusUCB scales well with large $K$ as compared with UCB1, EXP3, MOSS, KL-UCB and Thompson Sampling since it is eliminating sub-optimal arms after some timesteps. 
We must also remember as the number of arms increases UCB1, UCB2, KL-UCB and UCB($\delta$) will take more time as all of these algorithms have to build their confidence set over all the arms which will take $O(K)$ time whereas algorithms like ClusUCB, UCB-Revisited and Median Elimination will take much lesser time as they keep on removing sub-optimal arm after each round. KL-UCB because of its calculation of the divergence function performs much slower as compared to ClusUCB. Also, ClusUCB can be used in the budgeted bandit setup since within a fixed horizon/budget $T$, the algorithm comes up with an optimal arm with an exponentially decreasing error probability as proved theoretically in this work. Further uses of this algorithm can be in the contextual bandit scenario whereby the clustering of arms can be done on the basis of the feature vectors of the arms and users.
%As $\Delta \rightarrow 0$ and $K$ becomes large Clustered UCB will perform better than KL-UCB. This type of cases frequently occur in web advertising scenarios where the $r_{i}$ are small and large number of ads are available in the pool.
%To check the regret of various algorithms an illustrative table is given below:-
%\newline
%A table showing the relevant regret bounds of some algorithms is shown in Appendix~\ref{App:C}.



%\newpage
%\subsubsection*{Acknowledgements}
%We are deeply thankful to Pratik Gajane, PhD scholar, INRIA SequeL/Orange labs for his insightful suggestions and many corrections.
%*I will acknowledge Pratik Gajane here.*
%Use unnumbered third level headings for the acknowledgements title.
%All acknowledgements go at the end of the paper.


\bibliographystyle{plainnat}
%\vspace*{-1cm}
\bibliography{biblio}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\newpage
\onecolumn
 \section*{Appendix}

\input{appendix}



\end{document}
