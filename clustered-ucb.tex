\documentclass[twoside]{article} 

\usepackage{macros}

\begin{document}

\twocolumn[

\aistatstitle{UCB with clustering and improved exploration}

\aistatsauthor{ }

\aistatsaddress{ } ]

\begin{abstract}

In this paper, we present a novel algorithm for the stochastic multi-armed bandit problem.
% , which achieves a better gap-dependent regret upper bound than UCB-Improved\cite{auer2010ucb} and MOSS\cite{audibert2009minimax} algorithms. 
% and thereby try to answer an open problem which has been raised before. 
Our proposed method, referred to as ClusUCB, partitions the arms into clusters and then follows the UCB-Improved strategy with aggressive exploration factors to eliminate sub-optimal arms as well as clusters. 
From a theoretical analysis, we establish that ClusUCB achieves a better gap-dependent regret upper bound than UCB-Improved\cite{auer2010ucb} and MOSS\cite{audibert2009minimax} algorithms. 
% We corroborate our findings both theoretically and empirically and show that by careful tuning of ClusUCB's exploration parameters, we can achieve a lower regret in comparison to MOSS and UCB-Improved. 
Further, numerical experiments on test-cases with small gaps between optimal and sub-optimal mean rewards show that ClusUCB results in lower cumulative regret than popular bandit algorithms such as UCB1\cite{auer2002finite}, UCB-Improved, MOSS, UCB-V\cite{audibert2009exploration}, Thompson Sampling\cite{agrawal2011analysis},Median Elimination\cite{even2006action}, KL-UCB\cite{garivier2011kl} and DMED\cite{honda2010asymptotically}.

%Old Abstract 27.8.2016

%In this paper, we present a novel algorithm which achieves a better upper bound on regret for the stochastic multi-armed bandit problem then some of the existing algorithms. Our proposed method clusters arms based on their average estimated payoff. This results in a more efficient elimination of arms within clusters as well as the deletion of weak clusters. We prove the regret upper bound  as 
%$\sum_{i\in B_{m}}\bigg (\max{\bigg\lbrace \bigg(\dfrac{32}{(\Delta_{i})^{3}}\bigg) ,\bigg(\dfrac{25\Delta_{i}}{(\Delta^{2})(0.16T\Delta^{2})^{2|B_{m}|^{2}\Delta/5}}\bigg)\bigg\rbrace} + \bigg(\Delta_{i}+\dfrac{32\log{(T\dfrac{\Delta_{i}^{4}}{16})}}{\Delta_{i}}\bigg)\bigg)$, $\Delta$ is the minimal gap between the means of the reward distributions of the optimal arm and a sub-optimal arm, $A$ is the set of arms, $T$ is the horizon and $\psi(m)$ is a parameter of the problem. This bound improves upon the existing algorithm of UCB-Revisited, MOSS, KL-UCB and UCB1 under certain cases. We corroborate our findings both theoretically and empirically and show that by sufficient tuning of parameter, we can achieve a lower regret than all the algorithms mentioned. In particular, in the test-cases where the arm set is dominated by small $\Delta$ and large $K$ we achieve a significantly lower cumulative regret than these algorithms.

%Old Abstract

%In this paper we achieve a lower regret bound by a novel method of using clustering of arms based on their average estimated payoff. By clustering of arms we divide the larger problem into smaller sub-problems and individually solve the sub-problems to arrive at a global solution. We prove that by this method we achieve a regret bound of the order of $O\bigg(K\log(KT\Delta^{2})\bigg)$ where $\Delta$ is the minimal gap between the means of the reward distributions of the optimal arm and a sub-optimal arm , $K$ is the number of arms and $T$ is the horizon. This improves upon UCB-Revisited having regret bound $O\bigg(K\dfrac{\log(T\Delta^{2})}{\Delta}\bigg)$ . This bound also improves upon UCB1 which has a regret of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$ and MOSS which has a minimax distribution free upper bound on the regret in the order of $O(\sqrt{TK})$ and a distribution dependent upper bound of $O\bigg(\dfrac{K\log(T\Delta^{2}/K)}{\Delta}\bigg)$. We corroborate our findings empirically and showed that by 
%sufficient tuning of some parameters we can achieve a lower regret than all the existing algorithms mentioned.
%\dots
%\keywords{Multi-Armed Bandit, UCB, Exploration-Exploitation, Clustering}
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{intro}

\section{Clustered UCB}
\label{sec:clusucb}
\input{algo}

%\newpage
\section{Main results}
\label{sec:results}
\input{results}

\section{Proof of Theorem 1}
\label{sec:proofTheorem}
\input{proofTheorem}

\section{Simulation experiments}
\label{sec:expts}
\input{expts}


\section{Conclusions and future work}
\label{sec:conclusions}
From a theoretical viewpoint, we conclude that the gap-dependent regret bound of ClusUCB is lower than MOSS and UCB-Improved. From the numerical experiments on settings with small gaps between optimal and sub-optimal mean rewards, we observed that ClusUCB outperforms several popular bandit algorithms. 
While we exhibited better regret bounds for ClusUCB, it would be interesting future research to improve the theoretical analysis of ClusUCB to achieve the gap-independent regret bound of MOSS and possibly also the gap-dependent bound conjectured in Section 2.4.3 of \cite{bubeck2012regret}.


\clearpage
\newpage
\bibliographystyle{plainnat}
%\vspace*{-1cm}
\bibliography{biblio}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\newpage
\onecolumn
\section*{Appendix}

\input{appendix}



\end{document}
