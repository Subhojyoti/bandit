\documentclass[twoside]{article} 

\usepackage{macros}

\begin{document}

\twocolumn[

\aistatstitle{UCB with clustering and improved exploration}

\aistatsauthor{ }

\aistatsaddress{ } ]

\begin{abstract}

In this paper, we present a novel algorithm for the stochastic multi-armed bandit problem
% , which achieves a better gap-dependent regret upper bound than UCB-Improved\cite{auer2010ucb} and MOSS\cite{audibert2009minimax} algorithms. 
% and thereby try to answer an open problem which has been raised before. 
Our proposed method, referred to as ClusUCB, partitions the arms into clusters and then follows the UCB-Improved strategy with aggressive exploration factors to eliminate sub-optimal arms as well as clusters. 
From a theoretical analysis, we establish that ClusUCB achieves a better gap-dependent regret upper bound than UCB-Improved\cite{auer2010ucb} and MOSS\cite{audibert2009minimax} algorithms. 
% We corroborate our findings both theoretically and empirically and show that by careful tuning of ClusUCB's exploration parameters, we can achieve a lower regret in comparison to MOSS and UCB-Improved. 
Further, numerical experiments on test-cases with small gaps between optimal and sub-optimal mean rewards show that ClusUCB results in lower cumulative regret than popular bandit algorithms such as UCB1\cite{auer2002finite}, UCB-Improved, UCB-V\cite{audibert2009exploration}, KL-UCB\cite{garivier2011kl} and DMED\cite{honda2010asymptotically}.

%Old Abstract 27.8.2016

%In this paper, we present a novel algorithm which achieves a better upper bound on regret for the stochastic multi-armed bandit problem then some of the existing algorithms. Our proposed method clusters arms based on their average estimated payoff. This results in a more efficient elimination of arms within clusters as well as the deletion of weak clusters. We prove the regret upper bound  as 
%$\sum_{i\in B_{m}}\bigg (\max{\bigg\lbrace \bigg(\dfrac{32}{(\Delta_{i})^{3}}\bigg) ,\bigg(\dfrac{25\Delta_{i}}{(\Delta^{2})(0.16T\Delta^{2})^{2|B_{m}|^{2}\Delta/5}}\bigg)\bigg\rbrace} + \bigg(\Delta_{i}+\dfrac{32\log{(T\dfrac{\Delta_{i}^{4}}{16})}}{\Delta_{i}}\bigg)\bigg)$, $\Delta$ is the minimal gap between the means of the reward distributions of the optimal arm and a sub-optimal arm, $A$ is the set of arms, $T$ is the horizon and $\psi(m)$ is a parameter of the problem. This bound improves upon the existing algorithm of UCB-Revisited, MOSS, KL-UCB and UCB1 under certain cases. We corroborate our findings both theoretically and empirically and show that by sufficient tuning of parameter, we can achieve a lower regret than all the algorithms mentioned. In particular, in the test-cases where the arm set is dominated by small $\Delta$ and large $K$ we achieve a significantly lower cumulative regret than these algorithms.

%Old Abstract

%In this paper we achieve a lower regret bound by a novel method of using clustering of arms based on their average estimated payoff. By clustering of arms we divide the larger problem into smaller sub-problems and individually solve the sub-problems to arrive at a global solution. We prove that by this method we achieve a regret bound of the order of $O\bigg(K\log(KT\Delta^{2})\bigg)$ where $\Delta$ is the minimal gap between the means of the reward distributions of the optimal arm and a sub-optimal arm , $K$ is the number of arms and $T$ is the horizon. This improves upon UCB-Revisited having regret bound $O\bigg(K\dfrac{\log(T\Delta^{2})}{\Delta}\bigg)$ . This bound also improves upon UCB1 which has a regret of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$ and MOSS which has a minimax distribution free upper bound on the regret in the order of $O(\sqrt{TK})$ and a distribution dependent upper bound of $O\bigg(\dfrac{K\log(T\Delta^{2}/K)}{\Delta}\bigg)$. We corroborate our findings empirically and showed that by 
%sufficient tuning of some parameters we can achieve a lower regret than all the existing algorithms mentioned.
%\dots
%\keywords{Multi-Armed Bandit, UCB, Exploration-Exploitation, Clustering}
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{intro}

\section{Clustered UCB}
\label{sec:clusucb}
\input{algo}

%\newpage
\section{Main results}
\label{sec:results}
\input{results}

\section{Proof Sketch}
\label{sec:proofSketch}
\input{proofSketch}

\section{Simulation experiments}
\label{sec:expts}
\input{expts}


\section{Conclusions and future work}
\label{sec:conclusions}
Our study concludes that theoretically the gap-dependent regret bound of ClusUCB is lower than MOSS and UCB-Improved. From the numerical experiments on setting with small gaps between optimal and sub-optimal mean rewards, we observed that ClusUCB outperforms UCB1, MOSS, UCB-Improved, UCB-V, KL-UCB, DMED and Median Elimination algorithms. 
While we exhibited better regret bounds for ClusUCB, it would be interesting future research to improve the theoretical analysis of ClusUCB to achieve the gap-independent regret bound of MOSS and possibly also the gap-dependent bound conjectured in Section 2.4.3 of \cite{bubeck2012regret}.
% under certain cases when the $\Delta_{i}$'s are small and $K$ is large. Such cases can frequently occur in web advertising scenarios where the $r_{i}$ are small and a large number of ads are available in the pool. For the critical case when $\Delta_{i:r_{i}<r*}, \forall i\in A$ are equal then ClusUCB performs better than UCB-Improved, UCB1, UCB-Tuned, KL-UCB, MOSS and Median Elimination with careful choosing  of parameters as shown empirically. 
% It is advantageous to employ ClusUCB as it scales well with the number of arms, in comparison to algorithms such as UCB1, UCB-Improved, MOSS, KL-UCB and Median Elimination. 
% This is because the latter algorithms We must also remember as the number of arms increases UCB1, MOSS, KL-UCB and DMED will take more time as all of these algorithms have to build their confidence set over all the arms whereas algorithms like ClusUCB, UCB-Improved and Median Elimination 
% will take much lesser time as they keep on removing sub-optimal arms after each round. Again KL-UCB because of its calculation of the divergence function performs much slower as compared to ClusUCB. Also, ClusUCB can be used in the budgeted bandit setup since within a fixed horizon/budget $T$, the algorithm comes up with an optimal arm with an exponentially decreasing error probability. Further uses of this algorithm can be in the contextual bandit scenario whereby the clustering of arms can be done on the basis of the feature vectors of the arms(advertisements) and users.
%As $\Delta \rightarrow 0$ and $K$ becomes large Clustered UCB will perform better than KL-UCB. This type of cases frequently occur in web advertising scenarios where the $r_{i}$ are small and large number of ads are available in the pool.
%To check the regret of various algorithms an illustrative table is given below:-
%\newline
%A table showing the relevant regret bounds of some algorithms is shown in Appendix~\ref{App:C}.


\clearpage
\newpage
\bibliographystyle{plainnat}
%\vspace*{-1cm}
\bibliography{biblio}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\newpage
\onecolumn
\section*{Appendix}

\input{appendix}



\end{document}
