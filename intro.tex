In this paper, we consider the stochastic multi-armed bandit problem, a classical problem in sequential decision making. In this setting,  a learning algorithm is provided with a set arms with reward distributions unknown to the algorithm. The learning proceeds in an iterative fashion, where in each round, the algorithm chooses an arm and receives a stochastic reward that is drawn from a stationary distribution specific to the arm selected.  
Given the goal of maximizing the cumulative reward, the learning algorithm faces the exploration-exploitation dilemma, i.e., in each round should the algorithm select the arm which has the highest observed mean reward so far 
(\textit{exploitation}), or should the algorithm choose a new arm to gain more knowledge of the true mean reward of the arms and thereby avert a sub-optimal greedy decision (\textit{exploration}). 

Formally, let $r_i$, $i=1,\ldots,K$ denote the mean rewards of the $K$ arms and $r^* = \max_i r_i$ the optimal mean reward. The objective in the stochastic bandit problem is to minimize the cumulative regret, which is defined as follows:
\begin{align*}
R_{T}=r^{*}T - \sum_{i\in A} r_{i}N_{i},
\end{align*}
where $T$ is the number of rounds, $N_{i}=\sum_{m=1}^n I(I_m=i)$ is the number of times the algorithm chose arm $i$ up to round $T$.
The expected regret of an algorithm after $T$ rounds can be written as
%\newline
%\newline
\begin{align*}
\E[R_{T}]= \sum_{i=1}^K \E[N_i] \Delta_i,
\end{align*}
where $\Delta_{i}=r^{*}-r_{i}$ denotes the gap between the means of the optimal arm and of the $i$-th arm. 


%The problem, gets more difficult when the $\Delta_{i}$'s are smaller and arm set is larger. Also let $\Delta=min_{i\in A}\Delta_{i}$, that is it is the minimum possible gap over all arms in $A$.
                                                                                                                                          

%NEW RELATED WORK AND CONTRIBUTION

An early work involving a bandit setup is \cite{thompson1933likelihood}, where the author deals the problem of choosing between two treatments to administer on patients who come in sequentially. Following the seminal work of Robbins \cite{robbins1952some}, bandit algorithms have been extensively studied in a variety of applications. 
From a theoretical standpoint, an asymptotic lower bound for the regret was established in \cite{lai1985asymptotically}. In particular, it was shown there that for any consistent allocation strategy, we have
$$\liminf_{T \to \infty}\frac{\E[R_{T}]}{\log T}\geq\sum_{\{i:r_{i}<r^{*}\}}\dfrac{(r^{*}-r_{i})}{D(p_{i}||p^{*})},$$
where $D(p_{i}||p^{*})$ is the Kullback-Leibler divergence between the reward densities $p_{i}$ and $p^{*}$, corresponding to arms with mean $r_{i}$ and $r^{*}$, respectively.

	There have been several algorithm with strong regret guarantees. The foremost among them is UCB1 \cite{auer2002finite}, which has a regret upper bound of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$, where $\Delta = \min_{i:\Delta_i>0} \Delta_i$. This result is asymptotically order-optimal for the class of distributions considered. However, the worst case regret of UCB1  can be as bad as $\Omega \bigg(\sqrt{TK\log T}\bigg)$ - see \cite{audibert2009minimax}.  In the latter reference, the authors propose the MOSS algorithm and establish that the worst case regret of MOSS is $O\bigg(\sqrt{TK}\bigg)$ which improves upon UCB1 by a factor of order $\sqrt{\log T}$ and a gap-dependent regret of $\sum_{i:\Delta_{i}>0}\dfrac{K\log(2+T\Delta_{i}^{2}/K)}{\Delta_{i}}$. However, in certain regimes, the gap-dependent regret bound of MOSS can be worse than UCB1\todos{What regime is this? Can you give a reference here?}. The UCB-Improved algorithm, proposed in \cite{auer2010ucb}, is a round-based algorithm variant of UCB1 that has a gap-dependent regret bound of $O\bigg(\dfrac{K\log T\Delta^{2}}{\Delta}\bigg)$ and a worst case regret of $O\bigg(\sqrt{TK\log K}\bigg)$. An algorithm is \textit{round-based} if it pulls all the arms equal number of times in each round and then proceeds to eliminate one or more arms that it identifies to be sub-optimal. 
	
	%But this algorithm performs worse empirically as stated in \cite{lattimore2015optimally}. \textit{In this work we try to address one of the open questions as raised in \cite{bubeck2012regret} that is to find an algorithm with regret always better than MOSS and UCB-Improved.}
	%
	%In this context of Multi-Armed Stochastic Bandit, we will also like to mention some of the variance based algorithm such as UCB-Normal(\cite{auer2002finite}) , UCB-Tuned(\cite{auer2002finite}) and UCB-Variance(\cite{audibert2009exploration}) which shows that variance-aware algorithms tend to perform better than the algorithms that don't(UCB1, UCB-Improved, MOSS). It can be shown that  when the variance of some sub-optimal arm is lower, a variance-aware algorithm detects it quickly thereby reducing  regret. \textit{We test our algorithm against such variance-aware techniques and do an empirical analysis.} 
	%
	%A discussion on some recent algorithms employing divergence based techniques must also be done. Firstly, the algorithm proposed in \cite{honda2010asymptotically} the authors come up with the algorithm Deterministic Minimum Empirical Divergence also called DMED$+$(as referred by \cite{garivier2011kl}) which is first order optimal(they only give asymptotic guarantees). This algorithm keeps a track of arms whose empirical mean are close to the optimal arm and takes help of large deviation ideas to find the optimal arm. Secondly, the more recent algorithm called KL-UCB(\cite{garivier2011kl}) using KL-Divergence comes up with an upper bound on regret as $\sum_{i:\Delta_{i}>0}\bigg(\dfrac{\Delta_{i}(1+\alpha)\log T}{D(r_{i},r^{*})}+C_{1}\log\log T+\dfrac{C_{2}(\alpha)}{T^{\beta(\alpha)}}\bigg)$ which is strictly better than UCB1 as we know from Pinsker's inequality $D(r_{i},r^{*}) > 2\Delta_{i}^{2}$. KL-UCB beats UCB1, MOSS and UCB-Tuned in various scenarios. \textit{We empirically test against this algorithm and show that in certain regimes our algorithm performs better than KL-UCB and DMED.}
	%
	%We also make a distinction between round-based algorithms like UCB-Improved, Successive Reject(\cite{audibert2010best}), Successive Elimination(\cite{even2006action}) and Median-Elimination(\cite{even2006action}) whereby the algorithm pulls all the arms equal number of times in each round/phase, then proceeds to eliminate some number of arms it identifies to be sub-optimal with high probability and this continues till you are left with one arm. \textit{Our algorithm is also a round based algorithm}. Finally, we must point out that our setup is strictly limited to stochastic scenario as opposed to adversarial setup as studied in the paper \cite{auer2002nonstochastic} where an adversary sets the reward for the arms for every timestep.
	
\section*{Our Work}

	We first analyze what are the main drawbacks of UCB-Improved as discussed in \cite{liu2016modification} before proposing our improvements. 
	
\begin{itemize}
\item \textbf{Early Exploration:} A significant number of pulls are spend in initial exploration, since UCB-Improved is pulling all the arms $n_{m}=\bigg\lceil \dfrac{ 2\log(T\tilde{\Delta}^{2}_{m})}{\tilde{\Delta}^{2}_{m}} \bigg\rceil$ for the $m$-th round where $\tilde{\Delta}_{m}$ is initialized at $1$ and halved after every round.
\item \textbf{Not an anytime algorithm:} Since the horizon $T$ has to be pre-specified to the algorithm, the algorithm cannot be stopped anytime because various properties(on which arms are eliminated and pulls calculated) might not hold when it is stopped prematurely.
\item \textbf{Conservative arm elimination:} In UCB-Improved an arm is only eliminated after $\tilde{\Delta}_{m}<\dfrac{\Delta_{i}}{2}$, for some sub-optimal arm $a_{i}$. When $K$ is large and in the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$'s are small this has significant disadvantage as it will wait till later rounds to eliminate the sub-optimal arms.
\end{itemize}
	To counter early exploration in \cite{liu2016modification} as well as in our algorithm we propose an exploration regulatory factor to control exploration. \textit{Our algorithm is also not an anytime algorithm}(neither is MOSS, UCB-Improved) and in this context we point out that knowledge of the horizon actually facilitates learning as it can exploit more information(as stated in \cite{lattimore2015optimally}). We also employ a couple of more strategies to bring down our regret as summarized below:-
	
\begin{itemize}
\item \textbf{Clustering and local exploration:} Partition the action space(arms) into small clusters, each having uniform set of arms. Perform limited local exploration in this partitions thereby reducing expected regret. This is a one-time clustering done at the start before beginning the rounds and the number of clusters is pre-specified to the algorithm.
\item \textbf{Arm Elimination:} Run an independent version of UCB-Improved inside each such clusters formed and eliminate sub-optimal arms within such clusters. We call this arm elimination condition.
\item \textbf{Cluster Elimination:} Select the best payoff arm from each cluster, consider them as representative of their clusters and run cluster elimination condition on them to remove the entire cluster.
\item \textbf{Exploration Regulatory Factor:} Introduce an exploration regulatory factor to control exploration so that in the later rounds we taper our exploration.
\item \textbf{Aggressive Elimination:} Since we have reduced the amount of exploration by dividing the larger problem into smaller sub-problems and decreasing the number of pulls, we need aggressive elimination to remove sub-optimal arms so that within the same number of rounds as UCB-Improved we can eliminate all the sub-optimal arms with high probability. We introduce parameters $\rho_{a}$ and $\rho_{s}$ which helps in aggressive arm elimination and cluster elimination respectively. We do a theoretical analysis of it and show that the introduction of this parameter reduces expected regret but increases risk. We also show how various choices of these two parameters affect the expected regret.
\end{itemize}

	Summarizing our contributions below:-
\begin{enumerate}
\item We propose a cluster based round-wise algorithm with two arm elimination conditions in each round.
\item We achieved a lower regret upper bound(Theorem1,table in Appendix F) than UCB-Improved(\cite{auer2010ucb}), UCB1(\cite{auer2002finite}) and  MOSS (\cite{audibert2009minimax} which we verify both theoretically and empirically.
\item Our algorithm also empirically compares well with DMED, UCB-Varinace and KL-UCB.
\item In the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$'s are small and $K$ is large which is encountered frequently in web-advertising domain (as stated in \cite{garivier2011kl}) this approach has a significant advantage over other methods.
\item The only parameter that needs to be pre-specified to the algorithm is the number of clusters to be formed but unlike KL-UCB our algorithm parameter $p$ is not distribution-specific and also our algorithm does not involve calculation of a complex, time consuming function like the divergence function of KL-UCB.
%though the authors specified in \cite{garivier2011kl} that for optimal result only one should use the divergence function specific to the type of distribution.
%\item We also provide a short discussion on what other applications our algorithm can be employed successfully.
\end{enumerate}
	
	The paper is organized as follows, in section $2$ we present notations and preliminary assumptions. In section $3$ we present the algorithm and discuss why it works. Section $4$ deals with all the proof including the proofs on regret bound. In section $5$ we present the experimental run of the algorithm and in section $6$ we conclude. 
