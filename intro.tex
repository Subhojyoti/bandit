In this paper, we address the stochastic multi-armed bandit problem, a classical problem in sequential decision making. Here, a learning agent is provided with a set of choices or actions, also called arms and it has to choose one arm in each timestep. After choosing an arm the agent receives a reward from the environment, which is an independent draw from a stationary distribution specific to the arm selected. The goal of the agent is to maximize the cumulative reward. The agent is oblivious to the mean of the distributions associated with each arm, denoted by $r_{i}$, including the optimal arm which will give it the best average reward, denoted by $r^{*}$. The agent records the cumulative reward it has collected for any arm divided by the number of pulls of that arm which is called the estimated mean reward of an arm denoted by $\hat{r}_{i}$. In each trial the agent faces the exploration-exploitation dilemma, that is, should the agent select the arm which has the highest observed mean reward till now (
exploitation), or should the agent explore other arms to gain more knowledge of the true mean reward of the arms and thereby avert a sub-optimal greedy decision (exploration). The objective in the bandit problem is to maximize cumulative reward which will lead to minimizing the expected cumulative regret. We define the regret$(R_{T})$ of an algorithm after $T$ trials as
%\newline
%\newline
\begin{align*}
R_{T}=r^{*}T - \sum_{i\in A} r_{i}\mathbb{E}[N_{i}]
\end{align*}
\todos{$R_T$ on LHS with an expectation on the RHS?}
%\hspace*{6em}$R_{T}=r^{*}T$ - $\sum_{i\in A} r_{i}E[N_{i}]$
%\newline
%\newline
where $N_{i}$ denotes the number of times the learning agent chooses arm $i$ within the first $T$ trials. We also define $\Delta_{i}=r^{*}-r_{i}$, that is the difference between the mean of the optimal arm and the $i$-th sub-optimal arm (for simplicity we assume that there is only one optimal arm which will give the highest payoff). The problem, gets more difficult when the $\Delta_{i}$'s are smaller and arm set is larger. Also let $\Delta=min_{i\in A}\Delta_{i}$, that is it is the minimum possible gap over all arms in $A$.
                                                                                                                                          

\subsection*{Related work}
\todos{This section is too long and lacks a qualitative comparison with Clus-UCB. If the target is one of the ML conferences, then this section has to restricted to a couple of paragraphs that compare Clus-UCB with closely related works, for e.g. UCB-revisited.}
Bandit problems have been extensively studied under various conditions. One of the first works can be seen in \cite{thompson1933likelihood}, which deals with choosing between two treatments to administer on patients who come in sequentially. Further studies by \cite{robbins1952some} and \cite{lai1985asymptotically}, established an asymptotic lower bound for the regret. Lai and Robbins proved in \cite{lai1985asymptotically} that for any allocation strategy and for any sub-optimal arm $i$, the regret is lower bounded by 
$$\lim_{T\rightarrow\infty} \inf\dfrac{\mathbb{E}[R_{T}]}{\log T}\geq
\sum_{i:r_{i}<r^{*}}\dfrac{(r^{*}-r_{i})}{D(p_{i}||p^{*})},$$
where $D(p_{i}||p^{*})$ is the Kullback-Leibler divergence over the reward density $p_{i}$ and $p^{*}$ over the arms having mean $r_{i}$ and $r^{*}$.

	Of the several algorithms mentioned in \cite{auer2002finite}, UCB1 has a regret upper bound of \\$O\bigg(\dfrac{K\log T}{\Delta}\bigg)$ whereas in the same paper the author's propose UCB2 algorithm which has a tighter regret bound than UCB1. UCB2 has a parameter $\alpha$ that needs to be tuned and its regret is upper bounded by $\sum_{i\in A}\bigg(\dfrac{(1+C_{1}(\alpha))\log T}{2\Delta_{i}}\bigg) + C_{2}(\alpha)$, where $C_{1}(\alpha)>0$ is a constant that can get arbitrarily small when $\alpha$ gets smaller but $C_{2}(\alpha)$ consequently starts increasing. Another type of strategy was first proposed by \cite{sutton1998reinforcement} called $\epsilon-$greedy strategy where the agent behaves greedily most of the time pulling the arm having highest $\hat{r}_{i},\forall i\in A$ and sometimes with a small probability $\epsilon$ it will try to explore by pulling a sub-optimal arm. In \cite{auer2002finite} they further  refined the same algorithm and proposed $\epsilon_{n}$-greedy with regret guarantee. For the 
$\epsilon_{n}$-greedy it is  proved that if the parameter $\epsilon$, is made a function over time, like $\epsilon_{t}=\dfrac{const.K}{d^{2}t}$, such that $0<\epsilon<1$, then the regret grows logarithmically $\bigg(\dfrac{K\log T}{d^{2}}\bigg)$. This algorithm performs well given that $0<d<\min_{i\in A}{\Delta_{i}}$ and for large $const$ value the result actually becomes stronger than UCB1. For further insight into various approaches in dealing with stochastic multi-armed bandit we refer the reader to \cite{bubeck2012regret}. In \cite{auer2010ucb} they prove an improved regret bounds for the algorithm UCB-Revisited in the order of $\sum_{i\in A}\bigg(const*\dfrac{K\log (T\Delta_{i}^{2})}{\Delta_{i}}\bigg)$ for very small $\Delta_{i}$ over a larger set of arms. This is a round based method, where in every round, all the arms are pulled an equal number of times, then based on certain conditions they eliminate some arms and this goes on till one arm is left. In this paper we refer to these type of algorithms 
as round based algorithms.
	
	Other prominent round based elimination algorithms include Successive Reject, Successive Elimination and Median Elimination. The Successive reject algorithm was proposed by  \cite{audibert2010best}, where they explore the pure exploration scenario in a fixed budget/horizon setup. Successive Reject tries to proceed in a phase-wise manner eliminating one arm after each phase. They try to bound the regret by defining two hardness parameter $H_{1}=\sum_{i=1}^{K}\frac{1}{\Delta_{i}^{2}}$ and $H_{2}=\max_{i\in A}{i\Delta_{i}^{-2}}$. However, knowledge of these two parameters beforehand is a difficult task so an online approach to estimate them is used in Adaptive UCB-E. The next algorithm called Successive Elimination proposed by \cite{even2006action} also proceeds by eliminating one arm after every round and the authors give PAC-guarantees for them. In PAC guarantee algorithms the learning agent comes up with an $\epsilon$-optimal arm with $\delta$ error probability. They also propose a further modification of 
Successive Elimination called Median Elimination which removes one half of the arms after every round. The sample complexity of Successive Elimination for any sub-optimal arm is bounded by $O\bigg(\sum_{\Delta_{i}>\epsilon}\dfrac{\log(\frac{K}{\delta\Delta_{i}})}{\Delta_{i}^{2}}+ \dfrac{N(\Delta,\epsilon)}{\epsilon^{2}}\log(\dfrac{N(\Delta,\epsilon)}{\delta})\bigg)$, where $N(\Delta,\epsilon)$ are the number of arms which are $\epsilon-$optimal whereas for Median Elimination the sample complexity for any sub-optimal arm is bounded by $O(\sum_{i=1}^{\log_{2}K}\dfrac{K}{\epsilon^{2}}\log(\dfrac{1}{\delta}))$, where $\delta$ and $\epsilon$ are the parameters defined before.  For further insight into various approaches in dealing with stochastic multi-armed bandit we refer the reader to \cite{bubeck2012regret}.
	
	It is also important to distinguish between two approaches in the UCB type algorithms. One approach uses explicit mean estimation using Chernoff Bounds for calculating the confidence bound whereas the other approach uses variance estimation using Bernstein or Bennett's Inequality to estimate the confidence bound. Such variance estimation is found in \cite{auer2002finite} for the UCB-Normal and UCB-Tuned algorithm and later in \cite{audibert2009exploration}  where they use Bernstein Inequality to build the confidence term for UCB-V algorithm. But UCB1, UCB2, MOSS, UCB-Revisited, KL-UCB and Median-Elimination uses no such variance estimation techniques. In our analysis also we use no variance estimation methods.
	
	Some of the more recent algorithm like MOSS and KL-UCB provides further refinements to the upper bound in the stochastic multi-armed bandit case. In \cite{audibert2009minimax} the MOSS(Minimax Optimal  Strategy in Stochastic case) algorithm achieves a distribution free upper bound on the regret as $const.\sqrt{TK}$ and the authors also propose a distribution dependent upper bound as \\$\sum_{i:\Delta_{i}>0}\dfrac{K\log(2+T\Delta_{i}^{2}/K)}{\Delta_{i}}$. In \cite{garivier2011kl} the authors propose KL-UCB which achieves an upper bound on regret as $\sum_{i:\Delta_{i}>0}\bigg(\dfrac{\Delta_{i}(1+\alpha)\log T}{D(r_{i},r^{*})}+C_{1}\log\log T+\dfrac{C_{2}(\alpha)}{T^{\beta(\alpha)}}\bigg)$ which is strictly better than UCB1 as we know from Pinsker's inequality $D(r_{i},r^{*}) > 2\Delta_{i}^{2}$. KL-UCB beats UCB1, MOSS and UCB-Tuned in various scenarios. Another algorithm that has been proposed in \cite{abbasi2011improved} called $UCB(\delta)$ creates a confidence interval which does not depend on timestep or 
on horizon T. The regret upper bound in this algorithm is given by $\sum_{i:\Delta_{i}>0}\bigg(3\Delta_{i}+\dfrac{16}{\Delta_{i}}\log\big(\dfrac{2K}{\Delta_{i}\delta}\big)\bigg)$ , where $\delta$ is the error probability as defined before. In \cite{honda2010asymptotically} the authors come up with the algorithm Deterministic Minimum Empirical Divergence also called DMED$+$(as referred by \cite{garivier2011kl}) which is first order optimal. This algorithm keeps a track of arms whose empirical mean are close to the optimal arm and takes help of large deviation ideas to find the optimal arm.
	
	Also, we mention the algorithm Exp3 in the adversarial bandit scenario. In the adversarial case, the agent is playing against an adversary who can arbitrarily set a reward on any arm. Algorithms used in adversarial case can be used in the stochastic scenario but not vice-versa. For Exp3 in \cite{auer2002nonstochastic} the authors achieve an upper bound of the order of $O(S\sqrt{KT\log KT})$, where $S$ is defined as the hardness of a problem. Finally, we mention one algorithm that involves Bayesian estimation technique called Thompson Sampling(TS) which uses a prior distribution over arms to estimate the posterior distribution and thereby converge on the optimal arm. In \cite{agrawal2011analysis} the authors come up with a regret upper bound of the order of $O([K(\dfrac{1}{\Delta^{2}})^{2}]\log T)$ for $K$-arm stochastic bandits using Thompson Sampling. 


\subsection*{Our contributions}
In this work, we present a novel method where we cluster the arms in each round based on the average estimated payoff $\hat{r}_{i}$. The purpose of this approach is to control exploration and at the same time exploit the clusters formed to create tight confidence bounds. To do this, we use hierarchical agglomerative clustering using the single-linkage clustering scheme as mentioned in \cite{friedman2001elements}. This is used to group the arms together at the start of each round. Then we deploy a two-pronged approach of exploring inside each clusters to eliminate sub-optimal arms and separately based on other conditions eliminating some weak clusters with all the arms inside it. After each round, at the beginning of the next round we again cluster arms after destroying the old cluster structures formed in the previous round. The logic behind clustering at the beginning of each round afresh is simply that at the initial rounds we have clusters formed with very bad purity level(we can imagine the purity level 
of a cluster being judged by how many arms having similar or $\epsilon_{m}-$close means $r_{i}$ getting clustered together), where in the later rounds we can have tight clusters with high purity level since now we have a better estimate of $\hat{r}_{i}, \forall i\in A$.

	The within cluster arm elimination condition and the entire cluster elimination are two complimentary strategies for speedy elimination of sub-optimal arms. Unlike UCB-Revisited from \cite{auer2010ucb} the within cluster arm elimination leads to dividing the larger problem of finding the optimal arm from the whole arm set into small sub-problems where in each such small clusters (say $M$ clusters, $M\leq K$) we have $M$ arm elimination conditions, thereby increasing the probability of deletion of a sub-optimal arm and hence reducing regret. Also by dividing it into small sub-problems the growth of our pulls in each round is always small given the optimal arm has survived which we ensure by tuning parameters appropriately.  We start with a very small cluster size limit(say 2) and double the limit after every round. We put an upper bound on this limit to be decided by the algorithm online so that the cluster size remains bounded and also since we are exploring each arm in a cluster based on its size, such 
bounds helps in controlling the exploration within a cluster because we know that single link clustering often forms large chains where the first and large elements may not at all be similar to each other. The entire cluster elimination conditions exploit the idea that if the optimal arm has survived till the later rounds it will be in a cluster of its own with no other sub-optimal arm and then we eliminate all sub-optimal clusters in a single round. This is essentially the same core concept of various round based strategies.
	
	Since, we have assumed that the horizon $T$ is known prior to the agent, hence we introduce the parameter $\psi(m)$ which can be sufficiently tuned to tide over very large or small horizons to enable a balanced exploration by the agent.
	%To achieve all the above guarantees we define three parameters in our algorithm, $D$ which is the maximum cluster size allowed, $\psi(m)$ which is a tuning parameter for exploration and $w\geq 2$ a weight factor which enables a faster elimination of sub-optimal arms.
\newline
Summarizing, the contributions of this research are listed below:

\begin{enumerate}
\item We propose a cluster based round-wise algorithm with two arm elimination conditions in each round.
%\item To our knowledge no other algorithm has proposed such UCB-type cluster based algorithm before.
\item We achieved a lower regret bound than UCB-Revisited(\cite{auer2010ucb}), UCB($\delta$) (\cite{abbasi2011improved}), UCB1(\cite{auer2002finite}), UCB2(\cite{auer2002finite}), UCB-Tuned(\cite{auer2002finite}), Median Elimination(\cite{even2006action}),  Exp3(\cite{auer2002nonstochastic}) and MOSS (\cite{audibert2009minimax} in scenarios when $\Delta$ is small and $K$ is large which is encountered frequently in web-advertising domain, which we verify empirically.
\todos{I don't see a rigourous theoretical justification for this claim anywhere in the paper. We have a regret bound in Theorem 1, but where is it shown that the bound in Thm 1 is better than UCB-Revisited(\cite{auer2010ucb}), UCB($\delta$) (\cite{abbasi2011improved}), UCB1(\cite{auer2002finite}), UCB2(\cite{auer2002finite}), UCB-Tuned(\cite{auer2002finite}), Median Elimination(\cite{even2006action}),  Exp3(\cite{auer2002nonstochastic}) and MOSS (\cite{audibert2009minimax}?}
\todos{\textit{``when $\Delta$ is small and $K$ is large which is encountered frequently in web-advertising domain...''} Can you give a reference that justifies low $\Delta$ in web-advertising?}
\item Our algorithm also compares well with DMED, DMED($+$) and KL-UCB.
\todos{Is the comparison empirical or theoretical? Please specify}
\item In the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$s are small, this approach has a significant advantage over other methods.
\item We also come up with an error bound to prove that the error probability decreases exponentially after each round.
\item Unlike KL-UCB our algorithm parameter $\psi(m)$ is not distribution-specific and also our algorithm does not involve calculation of a complex, time consuming function like the divergence function of KL-UCB.
%though the authors specified in \cite{garivier2011kl} that for optimal result only one should use the divergence function specific to the type of distribution.
%\item We also provide a short discussion on what other applications our algorithm can be employed successfully.
\end{enumerate}

The paper is organized as follows, in section $5$ we present the algorithm and in section $6$ we discuss the algorithm and why it works. Section $7$ deals with all the proof including the proofs on regret bound and error bound. In section $8$ we present the experimental run of the algorithm and in section $9$ we conclude. 
