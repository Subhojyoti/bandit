In this paper, we consider the stochastic multi-armed bandit problem, a classical problem in sequential decision making. In this setting,  a learning algorithm is provided with a set arms with reward distributions unknown to the algorithm. The learning proceeds in an iterative fashion, where in each round, the algorithm chooses an arm and receives a stochastic reward that is drawn from a stationary distribution specific to the arm selected.  
Given the goal of maximizing the cumulative reward, the learning algorithm faces the exploration-exploitation dilemma, i.e., in each round should the algorithm select the arm which has the highest observed mean reward so far 
(\textit{exploitation}), or should the algorithm choose a new arm to gain more knowledge of the true mean reward of the arms and thereby avert a sub-optimal greedy decision (\textit{exploration}). 

Formally, let $r_i$, $i=1,\ldots,K$ denote the mean rewards of the $K$ arms and $r^* = \max_i r_i$ the optimal mean reward. The objective in the stochastic bandit problem is to minimize the cumulative regret, which is defined as follows:
\begin{align*}
R_{T}=r^{*}T - \sum_{i\in A} r_{i}N_{i},
\end{align*}
where $T$ is the number of rounds, $N_{i}=\sum_{m=1}^n I(I_m=i)$ is the number of times the algorithm chose arm $i$ up to round $T$.
The expected regret of an algorithm after $T$ rounds can be written as
%\newline
%\newline
\begin{align*}
\E[R_{T}]= \sum_{i=1}^K \E[N_i] \Delta_i,
\end{align*}
where $\Delta_{i}=r^{*}-r_{i}$ denotes the gap between the means of the optimal arm and of the $i$-th arm. 


%The problem, gets more difficult when the $\Delta_{i}$'s are smaller and arm set is larger. Also let $\Delta=min_{i\in A}\Delta_{i}$, that is it is the minimum possible gap over all arms in $A$.
                                                                                                                                          

%NEW RELATED WORK AND CONTRIBUTION

An early work involving a bandit setup is \cite{thompson1933likelihood}, where the author deals the problem of choosing between two treatments to administer on patients who come in sequentially. Following the seminal work of Robbins \cite{robbins1952some}, bandit algorithms have been extensively studied in a variety of applications. 
From a theoretical standpoint, an asymptotic lower bound for the regret was established in \cite{lai1985asymptotically}. In particular, it was shown there that for any consistent allocation strategy, we have
$$\liminf_{T \to \infty}\frac{\E[R_{T}]}{\log T}\geq\sum_{\{i:r_{i}<r^{*}\}}\dfrac{(r^{*}-r_{i})}{D(p_{i}||p^{*})},$$
where $D(p_{i}||p^{*})$ is the Kullback-Leibler divergence between the reward densities $p_{i}$ and $p^{*}$, corresponding to arms with mean $r_{i}$ and $r^{*}$, respectively.

	There have been several algorithm with strong regret guarantees. The foremost among them is UCB1 \cite{auer2002finite}, which has a regret upper bound of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$, where $\Delta = \min_{i:\Delta_i>0} \Delta_i$. This result is asymptotically order-optimal for the class of distributions considered. However, the worst case regret of UCB1  can be as bad as $\Omega \bigg(\sqrt{TK\log T}\bigg)$ - see \cite{audibert2009minimax}.  In the latter reference, the authors propose the MOSS algorithm and establish that the worst case regret of MOSS is $O\bigg(\sqrt{TK}\bigg)$ which improves upon UCB1 by a factor of order $\sqrt{\log T}$ and a gap-dependent regret of $\sum_{i:\Delta_{i}>0}\dfrac{K\log(2+T\Delta_{i}^{2}/K)}{\Delta_{i}}$. However, in certain regimes, the gap-dependent regret bound of MOSS can be worse than UCB1\todos{What regime is this? Can you give a reference here?}. The UCB-Improved algorithm, proposed in \cite{auer2010ucb}, is a round-based algorithm variant of UCB1 that 
has a gap-dependent regret bound of $O\bigg(\dfrac{K\log T\Delta^{2}}{\Delta}\bigg)$ and a worst case regret of $O\bigg(\sqrt{TK\log K}\bigg)$. An algorithm is \textit{round-based} if it pulls all the arms equal number of times in each round and then proceeds to eliminate one or more arms that it identifies to be sub-optimal. 
	
	%But this algorithm performs worse empirically as stated in \cite{lattimore2015optimally}. \textit{In this work we try to address one of the open questions as raised in \cite{bubeck2012regret} that is to find an algorithm with regret always better than MOSS and UCB-Improved.}
	%
	%In this context of Multi-Armed Stochastic Bandit, we will also like to mention some of the variance based algorithm such as UCB-Normal(\cite{auer2002finite}) , UCB-Tuned(\cite{auer2002finite}) and UCB-Variance(\cite{audibert2009exploration}) which shows that variance-aware algorithms tend to perform better than the algorithms that don't(UCB1, UCB-Improved, MOSS). It can be shown that  when the variance of some sub-optimal arm is lower, a variance-aware algorithm detects it quickly thereby reducing  regret. \textit{We test our algorithm against such variance-aware techniques and do an empirical analysis.} 
	%
	%A discussion on some recent algorithms employing divergence based techniques must also be done. Firstly, the algorithm proposed in \cite{honda2010asymptotically} the authors come up with the algorithm Deterministic Minimum Empirical Divergence also called DMED$+$(as referred by \cite{garivier2011kl}) which is first order optimal(they only give asymptotic guarantees). This algorithm keeps a track of arms whose empirical mean are close to the optimal arm and takes help of large deviation ideas to find the optimal arm. Secondly, the more recent algorithm called KL-UCB(\cite{garivier2011kl}) using KL-Divergence comes up with an upper bound on regret as $\sum_{i:\Delta_{i}>0}\bigg(\dfrac{\Delta_{i}(1+\alpha)\log T}{D(r_{i},r^{*})}+C_{1}\log\log T+\dfrac{C_{2}(\alpha)}{T^{\beta(\alpha)}}\bigg)$ which is strictly better than UCB1 as we know from Pinsker's inequality $D(r_{i},r^{*}) > 2\Delta_{i}^{2}$. KL-UCB beats UCB1, MOSS and UCB-Tuned in various scenarios. \textit{We empirically test against this algorithm 
% and show that in certain regimes our algorithm performs better than KL-UCB and DMED.}
	%
	%We also make a distinction between round-based algorithms like UCB-Improved, Successive Reject(\cite{audibert2010best}), Successive Elimination(\cite{even2006action}) and Median-Elimination(\cite{even2006action}) whereby the algorithm pulls all the arms equal number of times in each round/phase, then proceeds to eliminate some number of arms it identifies to be sub-optimal with high probability and this continues till you are left with one arm. \textit{Our algorithm is also a round based algorithm}. Finally, we must point out that our setup is strictly limited to stochastic scenario as opposed to adversarial setup as studied in the paper \cite{auer2002nonstochastic} where an adversary sets the reward for the arms for every timestep.
	
\paragraph{Our Work}
As mentioned in a recent work \cite{liu2016modification}, UCB-Improved has two shortcomings: 	
\begin{inparaenum}[\bfseries (i)]
\item a significant number of pulls are spent in early exploration, since each round $m$ of UCB-Improved involves pulling every arm an identical $n_{m}=\bigg\lceil \dfrac{ 2\log(T\tilde{\Delta}^{2}_{m})}{\tilde{\Delta}^{2}_{m}} \bigg\rceil$ number of times. The quantity $\tilde{\Delta}_{m}$ is initialized to $1$ and halved after every round.
\item In UCB-Improved, arms are eliminated conservatively, i.e, only after $\tilde{\Delta}_{m}<\dfrac{\Delta_{i}}{2}$, the sub-optimal arm $i$ is discarded. This is disadvantageous when $K$ is large and the gaps are identical ($r_{1}=r_{2}=..=r_{K-1}<r^{*}$) and small.
\end{inparaenum}
% 	To counter early exploration in \cite{liu2016modification} as well as in our algorithm we propose an exploration regulatory factor to control exploration. \textit{Our algorithm is also not an anytime algorithm}(neither is MOSS, UCB-Improved) and in this context we point out that knowledge of the horizon actually facilitates learning as it can exploit more information(as stated in \cite{lattimore2015optimally}). We also employ a couple of more strategies to bring down our regret as summarized below:-

We propose a variant of UCB algorithm (henceforth referred to as ClusUCB) that incorporates clustering and an improved exploration scheme. ClusUCB is a round-based algorithm that
starts with a partition of the arms into small clusters, each having uniform set of arms. 
The clustering is done at the start with a pre-specified the number of clusters. 
Each round of ClusUCB involves both (individual) arm elimination as well as cluster elimiation. While the conditions governing the arm and cluster eliminations are inspired by UCB-Improved, though the exploration factors governing these conditions are relatively more aggressive than that in UCB-Improved. 

\begin{table}
\caption{Gap-dependent regret bounds for different bandit algorithms}
\label{tab:regret-bds}
\begin{center}
\begin{tabular}{|c|c|}
\toprule
Algorithm  & Upper bound \\
\midrule
UCB1         &$O\left(\dfrac{K\log T}{\Delta}\right)$ \\\midrule
UCB-Improved &$O\left(\dfrac{K\log (T\Delta^{2})}{\Delta}\right)$ \\\midrule
MOSS	     &$O\left(\dfrac{K\log\left(T\Delta^{2}/K\right)}{\Delta}\right)$\\\midrule
ClusUCB      &$O\left(\dfrac{K\log\left(\dfrac{T\Delta^{2}}{\sqrt{\log (KT)}}\right)}{\Delta}\right)$\\\bottomrule
\end{tabular}
\end{center}
\end{table}

From a theoretical analysis that involves carefully setting the exploration factors for arm and cluster elimination conditions, we observe that the gap-dependent regret of ClusUCB is better than that of UCB-Improved (see Table \ref{tab:regret-bds}). On the other hand, the gap-independent bound of ClusUCB is of the same order as UCB-Improved, i.e., $O\left(\sqrt{KT\log K}\right)$. While ClusUCB is not able to match the gap-independent bound of $O(\sqrt{KT})$ for MOSS, the gap-dependent bound of ClusUCB is better than MOSS when $\sqrt{\log (KT)} > K$.

On a setup with $r_{1}=r_{2}=..=r_{K-1}<r^{*}$, small $\Delta_{i}$'s and large $K$, we observe empirically that ClusUCB outperforms UCB-Improved\cite{auer2010ucb} and MOSS\cite{audibert2009minimax} as well as other popular stochastic bandit algorithms such as DMED\cite{honda2010asymptotically}, UCB-V\cite{audibert2009exploration} and KL-UCB\cite{garivier2011kl}.

% 	Summarizing our contributions below:-
% \begin{enumerate}
% \item We propose a cluster based round-wise algorithm with two arm elimination conditions in each round.
% \item We achieved a lower regret upper bound(Theorem1,table in Appendix F) than UCB-Improved(\cite{auer2010ucb}), UCB1(\cite{auer2002finite}) and  MOSS (\cite{audibert2009minimax} which we verify both theoretically and empirically.
% \item Our algorithm also empirically compares well with DMED, UCB-Varinace and KL-UCB.
% \item In the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$'s are small and $K$ is large which is encountered frequently in web-advertising domain (as stated in \cite{garivier2011kl}) this approach has a significant advantage over other methods.
% \item The only parameter that needs to be pre-specified to the algorithm is the number of clusters to be formed but unlike KL-UCB our algorithm parameter $p$ is not distribution-specific and also our algorithm does not involve calculation of a complex, time consuming function like the divergence function of KL-UCB.
% %though the authors specified in \cite{garivier2011kl} that for optimal result only one should use the divergence function specific to the type of distribution.
% %\item We also provide a short discussion on what other applications our algorithm can be employed successfully.
% \end{enumerate}
	
The rest of the paper is organized as follows: In Section \ref{sec:clusucb}, we present the ClusUCB algorithm. In Section \ref{sec:results}, we present the associated regret bounds and sketch the proofs in Section \ref{sec:proofSketch}. In Section \ref{sec:expts}, we present the numerical experiments and provide concluding remarks in Section \ref{sec:conclusions}. 
