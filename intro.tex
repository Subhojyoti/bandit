In this paper, we address the stochastic multi-armed bandit problem, a classical problem in sequential decision making. Here, a learning agent is provided with a set of choices or actions, also called arms and it has to choose one arm in each timestep. After choosing an arm the agent receives a reward from the environment, which is an independent draw from a stationary distribution specific to the arm selected. The goal of the agent is to maximize the cumulative reward. The agent is oblivious to the mean of the distributions associated with each arm, denoted by $r_{i}$, including the optimal arm which will give it the best average reward, denoted by $r^{*}$. The agent records the cumulative reward it has collected for any arm divided by the number of pulls of that arm which is called the estimated mean reward of an arm denoted by $\hat{r}_{i}$. In each trial the agent faces the exploration-exploitation dilemma, that is, should the agent select the arm which has the highest observed mean reward till now (
exploitation), or should the agent explore other arms to gain more knowledge of the true mean reward of the arms and thereby avert a sub-optimal greedy decision (exploration). The objective in the bandit problem is to maximize cumulative reward which will lead to minimizing the expected cumulative regret. We define the expected regret $(R_{T})$ of an algorithm after $T$ trials as
%\newline
%\newline
\begin{align*}
\mathbb{E}[R_{T}]=r^{*}T - \sum_{i\in A} r_{i}\mathbb{E}[N_{i}]
\end{align*}
%\todos{$R_T$ on LHS with an expectation on the RHS? Change Done(Subho)}
%\hspace*{6em}$R_{T}=r^{*}T$ - $\sum_{i\in A} r_{i}E[N_{i}]$
%\newline
%\newline
where $N_{i}$ denotes the number of times the learning agent chooses arm $i$ within the first $T$ trials. We also define $\Delta_{i}=r^{*}-r_{i}$, that is the difference between the mean of the optimal arm and the $i$-th sub-optimal arm (for simplicity we assume that there is only one optimal arm which will give the highest payoff). The problem, gets more difficult when the $\Delta_{i}$'s are smaller and arm set is larger. Also let $\Delta=min_{i\in A}\Delta_{i}$, that is it is the minimum possible gap over all arms in $A$.
                                                                                                                                          

%OLD RELATED WORK AND CONTRIBUTION

%\subsection*{Related work}
%\todos{This section is too long and lacks a qualitative comparison with Clus-UCB. If the target is one of the ML conferences, then this section has to restricted to a couple of paragraphs that compare Clus-UCB with closely related works, for e.g. UCB-revisited.}
%Bandit problems have been extensively studied under various conditions. One of the first works can be seen in \cite{thompson1933likelihood}, which deals with choosing between two treatments to administer on patients who come in sequentially. Further studies by \cite{robbins1952some} and \cite{lai1985asymptotically}, established an asymptotic lower bound for the regret. Lai and Robbins proved in \cite{lai1985asymptotically} that for any allocation strategy and for any sub-optimal arm $i$, the regret is lower bounded by 
%$$\lim_{T\rightarrow\infty} \inf\dfrac{\mathbb{E}[R_{T}]}{\log T}\geq
%\sum_{i:r_{i}<r^{*}}\dfrac{(r^{*}-r_{i})}{D(p_{i}||p^{*})},$$
%where $D(p_{i}||p^{*})$ is the Kullback-Leibler divergence over the reward density $p_{i}$ and $p^{*}$ over the arms having mean $r_{i}$ and $r^{*}$.
%
%	Of the several algorithms mentioned in \cite{auer2002finite}, UCB1 has a regret upper bound of \\$O\bigg(\dfrac{K\log T}{\Delta}\bigg)$ whereas in the same paper the author's propose UCB2 algorithm which has a tighter regret bound than UCB1. UCB2 has a parameter $\alpha$ that needs to be tuned and its regret is upper bounded by $\sum_{i\in A}\bigg(\dfrac{(1+C_{1}(\alpha))\log T}{2\Delta_{i}}\bigg) + C_{2}(\alpha)$, where $C_{1}(\alpha)>0$ is a constant that can get arbitrarily small when $\alpha$ gets smaller but $C_{2}(\alpha)$ consequently starts increasing. Another type of strategy was first proposed by \cite{sutton1998reinforcement} called $\epsilon-$greedy strategy where the agent behaves greedily most of the time pulling the arm having highest $\hat{r}_{i},\forall i\in A$ and sometimes with a small probability $\epsilon$ it will try to explore by pulling a sub-optimal arm. In \cite{auer2002finite} they further  refined the same algorithm and proposed $\epsilon_{n}$-greedy with regret guarantee. For the 
%$\epsilon_{n}$-greedy it is  proved that if the parameter $\epsilon$, is made a function over time, like $\epsilon_{t}=\dfrac{const.K}{d^{2}t}$, such that $0<\epsilon<1$, then the regret grows logarithmically $\bigg(\dfrac{K\log T}{d^{2}}\bigg)$. This algorithm performs well given that $0<d<\min_{i\in A}{\Delta_{i}}$ and for large $const$ value the result actually becomes stronger than UCB1. For further insight into various approaches in dealing with stochastic multi-armed bandit we refer the reader to \cite{bubeck2012regret}. In \cite{auer2010ucb} they prove an improved regret bounds for the algorithm UCB-Revisited in the order of $\sum_{i\in A}\bigg(const*\dfrac{K\log (T\Delta_{i}^{2})}{\Delta_{i}}\bigg)$ for very small $\Delta_{i}$ over a larger set of arms. This is a round based method, where in every round, all the arms are pulled an equal number of times, then based on certain conditions they eliminate some arms and this goes on till one arm is left. In this paper we refer to these type of algorithms 
%as round based algorithms.
%	
%	Other prominent round based elimination algorithms include Successive Reject, Successive Elimination and Median Elimination. The Successive reject algorithm was proposed by  \cite{audibert2010best}, where they explore the pure exploration scenario in a fixed budget/horizon setup. Successive Reject tries to proceed in a phase-wise manner eliminating one arm after each phase. They try to bound the regret by defining two hardness parameter $H_{1}=\sum_{i=1}^{K}\frac{1}{\Delta_{i}^{2}}$ and $H_{2}=\max_{i\in A}{i\Delta_{i}^{-2}}$. However, knowledge of these two parameters beforehand is a difficult task so an online approach to estimate them is used in Adaptive UCB-E. The next algorithm called Successive Elimination proposed by \cite{even2006action} also proceeds by eliminating one arm after every round and the authors give PAC-guarantees for them. In PAC guarantee algorithms the learning agent comes up with an $\epsilon$-optimal arm with $\delta$ error probability. They also propose a further modification of 
%Successive Elimination called Median Elimination which removes one half of the arms after every round. The sample complexity of Successive Elimination for any sub-optimal arm is bounded by $O\bigg(\sum_{\Delta_{i}>\epsilon}\dfrac{\log(\frac{K}{\delta\Delta_{i}})}{\Delta_{i}^{2}}+ \dfrac{N(\Delta,\epsilon)}{\epsilon^{2}}\log(\dfrac{N(\Delta,\epsilon)}{\delta})\bigg)$, where $N(\Delta,\epsilon)$ are the number of arms which are $\epsilon-$optimal whereas for Median Elimination the sample complexity for any sub-optimal arm is bounded by $O(\sum_{i=1}^{\log_{2}K}\dfrac{K}{\epsilon^{2}}\log(\dfrac{1}{\delta}))$, where $\delta$ and $\epsilon$ are the parameters defined before.  For further insight into various approaches in dealing with stochastic multi-armed bandit we refer the reader to \cite{bubeck2012regret}.
%	
%	It is also important to distinguish between two approaches in the UCB type algorithms. One approach uses explicit mean estimation using Chernoff Bounds for calculating the confidence bound whereas the other approach uses variance estimation using Bernstein or Bennett's Inequality to estimate the confidence bound. Such variance estimation is found in \cite{auer2002finite} for the UCB-Normal and UCB-Tuned algorithm and later in \cite{audibert2009exploration}  where they use Bernstein Inequality to build the confidence term for UCB-V algorithm. But UCB1, UCB2, MOSS, UCB-Revisited, KL-UCB and Median-Elimination uses no such variance estimation techniques. In our analysis also we use no variance estimation methods.
%	
%	Some of the more recent algorithm like MOSS and KL-UCB provides further refinements to the upper bound in the stochastic multi-armed bandit case. In \cite{audibert2009minimax} the MOSS(Minimax Optimal  Strategy in Stochastic case) algorithm achieves a distribution free upper bound on the regret as $const.\sqrt{TK}$ and the authors also propose a distribution dependent upper bound as \\$\sum_{i:\Delta_{i}>0}\dfrac{K\log(2+T\Delta_{i}^{2}/K)}{\Delta_{i}}$. In \cite{garivier2011kl} the authors propose KL-UCB which achieves an upper bound on regret as $\sum_{i:\Delta_{i}>0}\bigg(\dfrac{\Delta_{i}(1+\alpha)\log T}{D(r_{i},r^{*})}+C_{1}\log\log T+\dfrac{C_{2}(\alpha)}{T^{\beta(\alpha)}}\bigg)$ which is strictly better than UCB1 as we know from Pinsker's inequality $D(r_{i},r^{*}) > 2\Delta_{i}^{2}$. KL-UCB beats UCB1, MOSS and UCB-Tuned in various scenarios. Another algorithm that has been proposed in \cite{abbasi2011improved} called $UCB(\delta)$ creates a confidence interval which does not depend on timestep or 
%on horizon T. The regret upper bound in this algorithm is given by $\sum_{i:\Delta_{i}>0}\bigg(3\Delta_{i}+\dfrac{16}{\Delta_{i}}\log\big(\dfrac{2K}{\Delta_{i}\delta}\big)\bigg)$ , where $\delta$ is the error probability as defined before. In \cite{honda2010asymptotically} the authors come up with the algorithm Deterministic Minimum Empirical Divergence also called DMED$+$(as referred by \cite{garivier2011kl}) which is first order optimal. This algorithm keeps a track of arms whose empirical mean are close to the optimal arm and takes help of large deviation ideas to find the optimal arm.
%	
%	Also, we mention the algorithm Exp3 in the adversarial bandit scenario. In the adversarial case, the agent is playing against an adversary who can arbitrarily set a reward on any arm. Algorithms used in adversarial case can be used in the stochastic scenario but not vice-versa. For Exp3 in \cite{auer2002nonstochastic} the authors achieve an upper bound of the order of $O(S\sqrt{KT\log KT})$, where $S$ is defined as the hardness of a problem. Finally, we mention one algorithm that involves Bayesian estimation technique called Thompson Sampling(TS) which uses a prior distribution over arms to estimate the posterior distribution and thereby converge on the optimal arm. In \cite{agrawal2011analysis} the authors come up with a regret upper bound of the order of $O([K(\dfrac{1}{\Delta^{2}})^{2}]\log T)$ for $K$-arm stochastic bandits using Thompson Sampling. 
%
%
%\subsection*{Our contributions}
%In this work, we present a novel method where we partition/cluster the arms at the start based on a user defined variable $p$. The purpose of this approach is to control exploration and at the same time exploit the clusters formed to create tight confidence bounds. Then we deploy a two-pronged approach of exploring inside each clusters to eliminate sub-optimal arms and separately based on other conditions eliminating some weak clusters with all the arms inside it. After each round, at the beginning of the next round we again cluster arms after destroying the old cluster structures formed in the previous  round. We keep this clusters fixed throughout the time horizon $T$. We allocate arms uniformly throughout these clusters with each cluster $s_{k}$ containing the same number of terms.
%
%%The logic behind clustering at the beginning of each round afresh is simply that at the initial rounds we have clusters formed with very bad purity level(we can imagine the purity level of a cluster being judged by how many arms having similar or $\epsilon_{m}-$close means $r_{i}$ getting clustered together), where in the later rounds we can have tight clusters with high purity level since now we have a better estimate of $\hat{r}_{i}, \forall i\in A$.
%
%	The within cluster arm elimination condition and the entire cluster elimination are two complimentary strategies for speedy elimination of sub-optimal arms. Unlike UCB-Revisited from \cite{auer2010ucb} the within cluster arm elimination leads to dividing the larger problem of finding the optimal arm from the whole arm set into small sub-problems where in each such small clusters (say $M$ clusters, $M\leq K$) we have $M$ arm elimination conditions, thereby increasing the probability of deletion of a sub-optimal arm and hence reducing regret. Also by dividing it into small sub-problems the growth of our pulls in each round is always small given the optimal arm has survived which we ensure by tuning parameters appropriately. The entire cluster elimination conditions exploit the idea that if the optimal arm has survived till the later rounds it will be the maximum payoff arm in a cluster and then we eliminate all sub-optimal clusters(that is clusters whose maximum payoff arm are sub-optimal) in a single round. This is essentially the same core concept of various round based strategies.
%%\todos{(Subho) Removed the introduction of $\psi(m)$ from Contributions}
%	%Since, we have assumed that the horizon $T$ is known prior to the agent, hence we introduce the parameter $\psi(m)$ which can be sufficiently tuned to tide over very large or small horizons to enable a balanced exploration by the agent.
%	%To achieve all the above guarantees we define three parameters in our algorithm, $D$ which is the maximum cluster size allowed, $\psi(m)$ which is a tuning parameter for exploration and $w\geq 2$ a weight factor which enables a faster elimination of sub-optimal arms.
%%\newline
%	
%	Summarizing, the contributions of this research are listed below:
%
%\begin{enumerate}
%\item We propose a cluster based round-wise algorithm with two arm elimination conditions in each round.
%%\item To our knowledge no other algorithm has proposed such UCB-type cluster based algorithm before.
%\item We achieved a lower regret bound than UCB-Revisited(\cite{auer2010ucb}), UCB($\delta$) (\cite{abbasi2011improved}), UCB1(\cite{auer2002finite}), UCB2(\cite{auer2002finite}), UCB-Tuned(\cite{auer2002finite}), Median Elimination(\cite{even2006action}),  Exp3(\cite{auer2002nonstochastic}) and MOSS (\cite{audibert2009minimax} in scenarios when $\Delta$ is small and $K$ is large which is encountered frequently in web-advertising domain (as stated in \cite{garivier2011kl}), which we verify empirically.
%\todos{I don't see a rigourous theoretical justification for this claim anywhere in the paper. We have a regret bound in Theorem 1, but where is it shown that the bound in Thm 1 is better than UCB-Revisited(\cite{auer2010ucb}), UCB($\delta$) (\cite{abbasi2011improved}), UCB1(\cite{auer2002finite}), UCB2(\cite{auer2002finite}), UCB-Tuned(\cite{auer2002finite}), Median Elimination(\cite{even2006action}),  Exp3(\cite{auer2002nonstochastic}) and MOSS (\cite{audibert2009minimax}?}
%%\todos{\textit{``when $\Delta$ is small and $K$ is large which is encountered frequently in web-advertising domain...''} Can you give a reference that justifies low $\Delta$ in web-advertising? (Subho) Reference given, rest of the contribution unchanged, to be decide after the regret bound is found}
%\item Our algorithm also empirically compares well with DMED, DMED($+$) and KL-UCB.
%%\todos{Is the comparison empirical or theoretical? Please specify . Done. written empirically, Subho}
%\item In the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$s are small, this approach has a significant advantage over other methods.
%\item We also come up with an error bound to prove that the error probability decreases exponentially after each round.
%\item Unlike KL-UCB our algorithm parameter $\psi(m)$ is not distribution-specific and also our algorithm does not involve calculation of a complex, time consuming function like the divergence function of KL-UCB.
%%though the authors specified in \cite{garivier2011kl} that for optimal result only one should use the divergence function specific to the type of distribution.
%%\item We also provide a short discussion on what other applications our algorithm can be employed successfully.
%\end{enumerate}

%NEW RELATED WORK AND CONTRIBUTION

\section*{Related work}

\par Bandit problems have been extensively studied under various conditions. One of the first works can be seen in \cite{thompson1933likelihood}, which deals with choosing between two treatments to administer on patients who come in sequentially. Further studies by \cite{robbins1952some} and \cite{lai1985asymptotically}, established an asymptotic lower bound for the regret. In \cite{lai1985asymptotically} it's shown that for any allocation strategy and for any sub-optimal arm $i$, the regret is lower bounded by 
$\lim_{T\rightarrow\infty} \inf\dfrac{\mathbb{E}[R_{T}]}{\log T}\geq\sum_{i:r_{i}<r^{*}}\dfrac{(r^{*}-r_{i})}{D(p_{i}||p^{*})},$
where $D(p_{i}||p^{*})$ is the Kullback-Leibler divergence over the reward density $p_{i}$ and $p^{*}$ over the arms having mean $r_{i}$ and $r^{*}$.

	There has been a large number of algorithms with strong regret guarantees, the foremost among them as mentioned in \cite{auer2002finite} called UCB1 has a regret upper bound of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$. This result is asymptotically order-optimal with respect to the class of algorithms mentioned in \cite{lai1985asymptotically}, but its worst case regret as shown in \cite{audibert2009minimax} can be as bad as $\Omega \bigg(\sqrt{TK\log T}\bigg)$ in certain regimes.  In the same paper(\cite{audibert2009minimax}) the authors come up with an algorithm called MOSS(Minimax Optimal  Strategy in Stochastic case) which suffers a worst case regret of $O\bigg(\sqrt{TK}\bigg)$ which improves upon UCB1 by a factor of order $\sqrt{\log T}$ and a gap-dependent regret of $\sum_{i:\Delta_{i}>0}\dfrac{K\log(2+T\Delta_{i}^{2}/K)}{\Delta_{i}}$. But MOSS also has its limitations whereby in certain regimes its gap-dependent regret bound is even worse than UCB1. The UCB-Improved algorithm bridged many of its gap by coming up with a gap-dependent regret bound of the order of $O\bigg(\dfrac{K\log T\Delta^{2}}{\Delta}\bigg)$ which improves upon UCB1 and a worst case regret of $O\bigg(\sqrt{TK\log K}\bigg)$. But this algorithm performs worse empirically as stated in \cite{lattimore2015optimally}. \textit{In this work we try to address one of the open questions as raised in \cite{bubeck2012regret} that is to find an algorithm with regret always better than MOSS and UCB-Improved.}
	
	In this context of Multi-Armed Stochastic Bandit, we will also like to mention some of the variance based algorithm such as UCB-Normal(\cite{auer2002finite}) , UCB-Tuned(\cite{auer2002finite}) and UCB-Variance(\cite{audibert2009exploration}) which shows that variance-aware algorithms tend to perform better than the algorithms that don't(UCB1, UCB-Improved, MOSS). It can be shown that  when the variance of some sub-optimal arm is lower, a variance-aware algorithm detects it quickly thereby reducing  regret. \textit{We test our algorithm against such variance-aware techniques and do an empirical analysis.} 
	
	A discussion on some recent algorithms employing divergence based techniques must also be done. Firstly, the algorithm proposed in \cite{honda2010asymptotically} the authors come up with the algorithm Deterministic Minimum Empirical Divergence also called DMED$+$(as referred by \cite{garivier2011kl}) which is first order optimal(they only give asymptotic guarantees). This algorithm keeps a track of arms whose empirical mean are close to the optimal arm and takes help of large deviation ideas to find the optimal arm. Secondly, the more recent algorithm called KL-UCB(\cite{garivier2011kl}) using KL-Divergence comes up with an upper bound on regret as $\sum_{i:\Delta_{i}>0}\bigg(\dfrac{\Delta_{i}(1+\alpha)\log T}{D(r_{i},r^{*})}+C_{1}\log\log T+\dfrac{C_{2}(\alpha)}{T^{\beta(\alpha)}}\bigg)$ which is strictly better than UCB1 as we know from Pinsker's inequality $D(r_{i},r^{*}) > 2\Delta_{i}^{2}$. KL-UCB beats UCB1, MOSS and UCB-Tuned in various scenarios. \textit{We empirically test against this algorithm and show that in certain regimes our algorithm performs better than KL-UCB and DMED.}
	
	We also make a distinction between round-based algorithms like UCB-Improved, Successive Reject(\cite{audibert2010best}), Successive Elimination(\cite{even2006action}) and Median-Elimination(\cite{even2006action}) whereby the algorithm pulls all the arms equal number of times in each round/phase, then proceeds to eliminate some number of arms it identifies to be sub-optimal with high probability and this continues till you are left with one arm. \textit{Our algorithm is also a round based algorithm}. Finally, we must point out that our setup is strictly limited to stochastic scenario as opposed to adversarial setup as studied in the paper \cite{auer2002nonstochastic} where an adversary sets the reward for the arms for every timestep.
	
\section*{Contribution}

	We first analyze what are the main drawbacks of UCB-Improved as discussed in \cite{liu2016modification} before proposing our improvements. 
	
\begin{itemize}
\item \textbf{Early Exploration:} A significant number of pulls are spend in initial exploration, since UCB-Improved is pulling all the arms $n_{m}=\bigg\lceil \dfrac{ 2\log(T\tilde{\Delta}^{2}_{m})}{\tilde{\Delta}^{2}_{m}} \bigg\rceil$ for the $m$-th round where $\tilde{\Delta}_{m}$ is initialized at $1$ and halved after every round.
\item \textbf{Not an anytime algorithm:} Since the horizon $T$ has to be pre-specified to the algorithm, the algorithm cannot be stopped anytime because various properties(on which arms are eliminated and pulls calculated) might not hold when it is stopped prematurely.
\item \textbf{Conservative arm elimination:} In UCB-Improved an arm is only eliminated after $\tilde{\Delta}_{m}<\dfrac{\Delta_{i}}{2}$, for some sub-optimal arm $a_{i}$. When $K$ is large and in the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$'s are small this has significant disadvantage as it will wait till later rounds to eliminate the sub-optimal arms.
\end{itemize}
	To counter early exploration in \cite{liu2016modification} as well as in our algorithm we propose an exploration regulatory factor to control exploration. \textit{Our algorithm is also not an anytime algorithm}(neither is MOSS, UCB-Improved) and in this context we point out that knowledge of the horizon actually facilitates learning as it can exploit more information(as stated in \cite{lattimore2015optimally}). We also employ a couple of more strategies to bring down our regret as summarized below:-
	
\begin{itemize}
\item \textbf{Clustering and local exploration:} Partition the action space(arms) into small clusters, each having uniform set of arms. Perform limited local exploration in this partitions thereby reducing expected regret. This is a one-time clustering done at the start before beginning the rounds and the number of clusters is pre-specified to the algorithm.
\item \textbf{Arm Elimination:} Run an independent version of UCB-Improved inside each such clusters formed and eliminate sub-optimal arms within such clusters. We call this arm elimination condition.
\item \textbf{Cluster Elimination:} Select the best payoff arm from each cluster, consider them as representative of their clusters and run cluster elimination condition on them to remove the entire cluster.
\item \textbf{Exploration Regulatory Factor:} Introduce an exploration regulatory factor to control exploration so that in the later rounds we taper our exploration.
\item \textbf{Aggressive Elimination:} Since we have reduced the amount of exploration by dividing the larger problem into smaller sub-problems and decreasing the number of pulls, we need aggressive elimination to remove sub-optimal arms so that within the same number of rounds as UCB-Improved we can eliminate all the sub-optimal arms with high probability. We introduce parameters $\rho_{a}$ and $\rho_{s}$ which helps in aggressive arm elimination and cluster elimination respectively. We do a theoretical analysis of it and show that the introduction of this parameter reduces expected regret but increases risk. We also show how various choices of these two parameters affect the expected regret.
\end{itemize}

	Summarizing our contributions below:-
\begin{enumerate}
\item We propose a cluster based round-wise algorithm with two arm elimination conditions in each round.
\item We achieved a lower regret upper bound(Theorem1,table in Appendix F) than UCB-Improved(\cite{auer2010ucb}), UCB1(\cite{auer2002finite}) and MOSS (\cite{audibert2009minimax} which we verify theoretically and empirically.
\item Our algorithm also empirically compares well with DMED, DMED($+$), UCB-Varinace and KL-UCB.
%\todos{Is the comparison empirical or theoretical? Please specify . Done. written empirically, Subho}
\item In the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$'s are small and $K$ is large which is encountered frequently in web-advertising domain (as stated in \cite{garivier2011kl}) this approach has a significant advantage over other methods.
\item The only parameter that needs to be pre-specified to the algorithm is the number of clusters to be formed but unlike KL-UCB our algorithm parameter $p$ is not distribution-specific and also our algorithm does not involve calculation of a complex, time consuming function like the divergence function of KL-UCB.
%though the authors specified in \cite{garivier2011kl} that for optimal result only one should use the divergence function specific to the type of distribution.
%\item We also provide a short discussion on what other applications our algorithm can be employed successfully.
\end{enumerate}
	
	The paper is organized as follows, in section $2$ we present notations and preliminary assumptions. In section $3$ we present the algorithm and discuss why it works. Section $4$ deals with all the proof including the proofs on regret bound. In section $5$ we present the experimental run of the algorithm and in section $6$ we conclude. 
